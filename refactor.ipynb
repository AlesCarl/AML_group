{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_Pgj7F7mKPYE",
        "AROS8JDQX2TL",
        "_iNaf_PSP-m8",
        "5Itwp0habDGF",
        "1s0WDrNHbJn4",
        "pRZcqd68bODB",
        "o04pJHaJbTSm",
        "uS2EPux5bmft",
        "idyg6lT5cCTa",
        "hK-nOIXNb1Y8",
        "ApCQHpy6cGm0",
        "O_AuisAogSUb"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Installation"
      ],
      "metadata": {
        "id": "_Pgj7F7mKPYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDA4Ch6WXJC9"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
        "!pip install open-clip-torch\n",
        "\n",
        "!pip install open3d\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"AML_group\"):\n",
        "    !git clone https://github.com/AlesCarl/AML_group.git\n",
        "else:\n",
        "    print(\"Repository giÃ  clonato.\")\n",
        "%cd AML_group\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "6LyyUROpKMQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import open_clip\n",
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "from itertools import permutations, product\n",
        "from Normalization import MeshNormalizer\n",
        "from utils import device, color_mesh\n",
        "from mesh import Mesh\n",
        "from render import Renderer\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SaK9SPJ9XLtJ",
        "outputId": "1ba1f09e-f8b4-4354-aaa7-daa8873f14c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warp 1.5.1 initialized:\n",
            "   CUDA Toolkit 12.6, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Highlighter Class"
      ],
      "metadata": {
        "id": "AROS8JDQX2TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm([hidden_dim]))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "        self.mlp = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.mlp:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xZft-SeZXzYD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "ZdoZUiilX-Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clip_model(clip_model):\n",
        "    device = 'cuda'\n",
        "    model, preprocess = clip.load(clip_model, device=device) # jit = True for better perfomance\n",
        "    return model\n",
        "\n",
        "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5, show=False, center_azim=0, center_elev=0, std=1, return_views=True, lighting=True, background=background)\n",
        "\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "def clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "    if n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if encoded_text.shape[0] > 1:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "        else:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "    elif n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            #print(augmented_image.shape)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "        loss= loss / n_augs\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
      ],
      "metadata": {
        "id": "zGgxV8xkYBvS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer Loop"
      ],
      "metadata": {
        "id": "PzXksQQ4YQoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(obj_path, learning_rate, n_layers, n_views, n_augs, prompt, augment_transform=None):\n",
        "    render_res = 224\n",
        "    n_iter = 1000\n",
        "    res = 224\n",
        "    output_dir = './output/'\n",
        "    clip_model = 'ViT-L/14'\n",
        "\n",
        "    input_dim = 3\n",
        "    hidden_dim = 256\n",
        "    output_dim = 2\n",
        "\n",
        "    Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "    #render = Renderer(dim=(render_res, render_res), background_image= None )\n",
        "    render = Renderer(dim=(render_res, render_res), background_image='./data/desk1.jpg')\n",
        "\n",
        "    mesh = Mesh(obj_path)\n",
        "    MeshNormalizer(mesh)()\n",
        "\n",
        "    # Initialize variables\n",
        "    background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "    # CLIP and augmentation transform\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    if augment_transform is None:\n",
        "        augment_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(res, scale=(0.9, 1)),\n",
        "            transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "            clip_normalizer\n",
        "        ])\n",
        "\n",
        "    # MLP and optimizer Settings\n",
        "    mlp = NeuralHighlighter(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "    # List of possible colors\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "    # Encode prompt with CLIP\n",
        "    model = get_clip_model(clip_model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    vertices = copy.deepcopy(mesh.vertices)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in tqdm(range(n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        sampled_mesh = mesh\n",
        "        color_mesh(pred_class, sampled_mesh, colors)\n",
        "        rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views, show=False, center_azim=0, center_elev=0, std=1, return_views=True, lighting=True, background=background) #,background=background)\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, model)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(output_dir, i, rendered_images)\n",
        "            with open(os.path.join(output_dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "\n",
        "    # save results\n",
        "    save_final_results(output_dir, 'final_mesh', mesh, mlp, vertices, colors, render, background) ##\n",
        "\n",
        "    return mlp"
      ],
      "metadata": {
        "id": "XvdLVtA1YTUa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Part"
      ],
      "metadata": {
        "id": "_iNaf_PSP-m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_path = 'data/horse.obj'\n",
        "\n",
        "learning_rate = 0.0001\n",
        "n_layers = 5\n",
        "n_views = 4\n",
        "n_augs = 4\n",
        "\n",
        "prompt= \"A 3D rendering of a Horse with highlighted Shoes.\"\n",
        "\n",
        "model_instance = optimize(obj_path, learning_rate, n_layers, n_views, n_augs, prompt)"
      ],
      "metadata": {
        "id": "hW6byH5pI_4O",
        "outputId": "961c47b2-e2a7-43b0-d784-3c49897a6e9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Background image path:  ./data/bg1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|ââââââââ                               | 185M/890M [00:05<00:20, 36.1MiB/s]\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Part"
      ],
      "metadata": {
        "id": "5Itwp0habDGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mesh 2 Point Cloud"
      ],
      "metadata": {
        "id": "1s0WDrNHbJn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso del file OBJ\n",
        "obj_path = 'data/horse.obj'\n",
        "\n",
        "# Funzione per caricare la mesh come TriangleMesh\n",
        "def load_obj_as_triangle_mesh(obj_path):\n",
        "    # Carica il file .obj\n",
        "    mesh = o3d.io.read_triangle_mesh(obj_path)\n",
        "    if mesh.is_empty():\n",
        "        raise ValueError(f\"La mesh nel file {obj_path} non Ã¨ stata caricata correttamente.\")\n",
        "    return mesh\n",
        "\n",
        "# Carica la mesh\n",
        "mesh = load_obj_as_triangle_mesh(obj_path)\n",
        "\n",
        "# Converte la mesh in una point cloud campionando punti uniformemente\n",
        "pcd = mesh.sample_points_uniformly(2048)\n",
        "\n",
        "# Imposta il colore nero per tutti i punti\n",
        "pcd.colors = o3d.utility.Vector3dVector(np.zeros((len(pcd.points), 3)))  # Colore nero: [0, 0, 0]\n",
        "\n",
        "# Esporta la point cloud in formato PLY\n",
        "# o3d.io.write_point_cloud(\"candle_PC.ply\", pcd)\n",
        "\n",
        "# Point cloud to mesh\n",
        "\n",
        "pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
        "\n",
        "with o3d.utility.VerbosityContextManager(\n",
        "        o3d.utility.VerbosityLevel.Debug) as cm:\n",
        "    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n",
        "        pcd, depth=9)\n",
        "print(mesh)\n",
        "\n",
        "o3d.io.write_triangle_mesh(\"APPROXIMATE.obj\", mesh)"
      ],
      "metadata": {
        "id": "nfV8ll1ibFGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15561893-cb4c-4397-c36e-771d4fc290b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Open3D DEBUG] Input Points / Samples: 2048 / 2042\n",
            "[Open3D DEBUG] #   Got kernel density: 0.011857032775878906 (s), 1221.26953125 (MB) / 1221.26953125 (MB) / 1234 (MB)\n",
            "[Open3D DEBUG] #     Got normal field: 0.009994983673095703 (s), 1221.5234375 (MB) / 1221.5234375 (MB) / 1234 (MB)\n",
            "[Open3D DEBUG] Point weight / Estimated Area: 5.832422e-04 / 1.194480e+00\n",
            "[Open3D DEBUG] #       Finalized tree: 0.023658037185668945 (s), 1221.78125 (MB) / 1221.78125 (MB) / 1234 (MB)\n",
            "[Open3D DEBUG] #  Set FEM constraints: 0.15161585807800293 (s), 1221.78125 (MB) / 1221.78125 (MB) / 1234 (MB)\n",
            "[Open3D DEBUG] #Set point constraints: 0.006356000900268555 (s), 1221.78125 (MB) / 1221.78125 (MB) / 1234 (MB)\n",
            "[Open3D DEBUG] Leaf Nodes / Active Nodes / Ghost Nodes: 154127 / 86912 / 89233\n",
            "[Open3D DEBUG] Memory Usage: 1221.781 MB\n",
            "[Open3D DEBUG] # Linear system solved: 0.13513898849487305 (s), 1221.78125 (MB) / 1221.78125 (MB) / 1234 (MB)\n",
            "[Open3D DEBUG] Got average: 0.0054340362548828125 (s), 1221.78125 (MB) / 1221.78125 (MB) / 1234 (MB)\n",
            "[Open3D DEBUG] Iso-Value: 5.045862e-01 = 1.033393e+03 / 2.048000e+03\n",
            "[Open3D DEBUG] #          Total Solve:       0.9 (s),    1238.2 (MB)\n",
            "TriangleMesh with 5657 points and 11306 triangles.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Point Cloud 2 Mesh"
      ],
      "metadata": {
        "id": "pRZcqd68bODB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pointToMesh(data, output_path):\n",
        "    single_data = data[0].cpu().numpy()\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(single_data)\n",
        "\n",
        "    # Stima delle normali\n",
        "    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
        "        radius=0.06,  # Raggio per la stima delle normali,\n",
        "        max_nn=80     # Numero massimo di vicini per stimare la normale\n",
        "    ))\n",
        "\n",
        "    # Orienta le normali in modo consistente\n",
        "    pcd.orient_normals_consistent_tangent_plane(k=100)\n",
        "\n",
        "    # Ricostruzione con\n",
        "    mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_alpha_shape(\n",
        "        pcd, 0.04\n",
        "    )\n",
        "\n",
        "    # Salva e visualizza la mesh\n",
        "    o3d.io.write_triangle_mesh(output_path, mesh)"
      ],
      "metadata": {
        "id": "58Kjfs3nbHvS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third Part"
      ],
      "metadata": {
        "id": "o04pJHaJbTSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AffordanceNet Class to handle the dataset"
      ],
      "metadata": {
        "id": "uS2EPux5bmft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pc_normalize(pc):\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc, centroid, m\n",
        "\n",
        "\n",
        "class AffordNetDataset(Dataset):\n",
        "    def __init__(self, data_dir, split):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "\n",
        "        self.load_data()\n",
        "        self.affordance = self.all_data[0][\"affordance\"]\n",
        "        return\n",
        "\n",
        "    def load_data(self):\n",
        "     self.all_data = []\n",
        "     with open(os.path.join(self.data_dir, 'full_shape_%s_data.pkl' % self.split), 'rb') as f:\n",
        "        temp_data = pkl.load(f)\n",
        "     for index, info in enumerate(temp_data):\n",
        "        if info[\"semantic class\"] == \"Scissors\":  # Filtra solo gli oggetti \"BOWL\"\n",
        "            temp_info = {}\n",
        "            temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "            temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "            temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "            temp_info[\"data_info\"] = info[\"full_shape\"] #Â vertici\n",
        "            self.all_data.append(temp_info)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_dict = self.all_data[index]\n",
        "        modelid = data_dict[\"shape_id\"]\n",
        "        modelcat = data_dict[\"semantic class\"]\n",
        "\n",
        "        data_info = data_dict[\"data_info\"]\n",
        "        model_data = data_info[\"coordinate\"].astype(np.float32)\n",
        "        labels = data_info[\"label\"]\n",
        "        for aff in self.affordance:\n",
        "            temp = labels[aff].astype(np.float32).reshape(-1, 1)\n",
        "            model_data = np.concatenate((model_data, temp), axis=1)\n",
        "\n",
        "        datas = model_data[:, :3]\n",
        "        targets = model_data[:, 3:]\n",
        "        datas, _, _ = pc_normalize(datas)\n",
        "\n",
        "        return datas, datas, targets, modelid, modelcat\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_random_sample(self):\n",
        "        index = random.randint(0, len(self.all_data) - 1)\n",
        "        return self.__getitem__(index)\n",
        "\n",
        "\n",
        "\n",
        "def build_dataset(data_dir, test=False):\n",
        "    test_set = AffordNetDataset(data_dir, 'train')\n",
        "    val_set = AffordNetDataset(data_dir, 'val')\n",
        "\n",
        "    # Seleziona i primi 5 campioni per il VAL set\n",
        "    ### val_indices = list(range(min(6, len(val_set)))) #Â uno in piÃ¹\n",
        "    val_indices = random.sample(range(len(val_set)), min(6, len(val_set))) # casuali\n",
        "\n",
        "    val_set = Subset(val_set, val_indices)\n",
        "\n",
        "    # Seleziona 10 campioni per il TEST set, a partire dal 7Â° (indice 6)\n",
        "    test_start_index = 6\n",
        "    test_indices = list(range(test_start_index, test_start_index + min(10, len(test_set) - test_start_index)))\n",
        "    test_set = Subset(test_set, test_indices)\n",
        "\n",
        "    # Ritorna i dataset in un dizionario\n",
        "    dataset_dict = dict(val_set=val_set, test_set=test_set)\n",
        "    return dataset_dict\n",
        "\n",
        "def build_loader(dataset_dict):\n",
        "    val_set = dataset_dict[\"val_set\"]\n",
        "    test_set = dataset_dict[\"test_set\"]\n",
        "\n",
        "    batch_size_factor = 1\n",
        "\n",
        "    test_loader = DataLoader(test_set, batch_size=1, shuffle=True, drop_last=False, num_workers=8)\n",
        "    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=8, drop_last=False)\n",
        "    #random_sample = val_set.dataset.get_random_sample() #Â new\n",
        "\n",
        "\n",
        "    loader_dict = dict(val_loader=val_loader, test_loader=test_loader) #Â new\n",
        "    return loader_dict"
      ],
      "metadata": {
        "id": "fZax0_YKbU6D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the mIOU"
      ],
      "metadata": {
        "id": "idyg6lT5cCTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_miou(predictions, ground_truths, threshold):\n",
        "    ground_truths = (ground_truths >= threshold).astype(int)  # Binarizza ground truth\n",
        "\n",
        "    batch_size, num_points = ground_truths.shape\n",
        "    iou_per_class = np.zeros((batch_size, 1))\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        pred = predictions[b, :]\n",
        "        gt = ground_truths[b, :]\n",
        "\n",
        "        # Calcola intersezione e unione\n",
        "        intersection = np.sum(pred * gt)\n",
        "        union = np.sum(pred + gt) - intersection\n",
        "\n",
        "        if union == 0:  # Evita divisione per zero\n",
        "            iou_per_class[b, 0] = np.nan  # Non valido se non ci sono punti\n",
        "        else:\n",
        "            iou_per_class[b, 0] = intersection / union\n",
        "\n",
        "    # Media su batch e classi\n",
        "    mean_iou = np.nanmean(iou_per_class)\n",
        "    return mean_iou"
      ],
      "metadata": {
        "id": "H2IN3MbpcEQ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset"
      ],
      "metadata": {
        "id": "hK-nOIXNb1Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_dir = '/content/drive/My Drive/full-shape'\n",
        "dataset_dir = '/content/drive/My Drive/ColabNotebooks'\n",
        "\n",
        "dataset = build_dataset(dataset_dir)\n",
        "\n",
        "loader = build_loader(dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "NYcuPRP3b2ym",
        "outputId": "1f1dad46-5931-47b5-d7ad-6a8744b47f7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "ApCQHpy6cGm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training + Validation\n"
      ],
      "metadata": {
        "id": "PDjXAm4yPDKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_instance = None\n",
        "\n",
        "\n",
        "learning_rate = 0.0001\n",
        "n_layers = 4\n",
        "n_views = 4\n",
        "n_augs = 3\n",
        "\n",
        "\n",
        "\n",
        "#prompt= \"A 3D rendering of a pair of scissors, emphasizing the handles, which are shaped for a one-handed grip\" # training di una sola\n",
        "prompt= \"A 3D rendering of a pair of scissors, emphasizing the ergonomic handles, which are shaped for a firm and comfortable one-handed grip\"\n",
        "\n",
        "\n",
        "\n",
        "# Ottieni il primo batch dal val_loader\n",
        "first_batch = next(iter(loader[\"val_loader\"]))\n",
        "\n",
        "data, data1, targets, modelid, modelcat = first_batch\n",
        "vertex = data\n",
        "obj_path = 'alpha.obj'\n",
        "pointToMesh(data, obj_path)\n",
        "\n",
        "model_instance = optimize(obj_path, learning_rate, n_layers, n_views, n_augs, prompt) # 1 solo training\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WJPW6K4XcIBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81114ce-3f9d-4403-d82b-db501f58e426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Background image path:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1000 [00:00<11:09,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.2412109375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|â         | 101/1000 [01:04<09:44,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.257255859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|ââ        | 201/1000 [02:09<08:41,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24457275390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|âââ       | 301/1000 [03:14<07:38,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.248316650390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|ââââ      | 401/1000 [04:20<06:37,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24991943359375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|âââââ     | 501/1000 [05:25<05:25,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.249744873046875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|ââââââ    | 601/1000 [06:30<04:21,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24816162109375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|âââââââ   | 701/1000 [07:36<03:15,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24906982421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|ââââââââ  | 801/1000 [08:41<02:10,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.250093994140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|âââââââââ | 901/1000 [09:46<01:04,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24994384765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 1000/1000 [10:52<00:00,  1.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Â nuovo eval\n",
        "\n",
        "\n",
        "model_instance.eval()  # imposta il modello in modalitÃ  valutazione\n",
        "\n",
        "all_mious = []\n",
        "\n",
        "with torch.no_grad():  # disabilita il calcolo dei gradienti per l'inferenza\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader[\"val_loader\"]):\n",
        "\n",
        "        if batch_idx == 1: #questo perchÃ¨ il primo valore lo uso per il train ( VA BENE ???????--- TODO ############)\n",
        "         continue\n",
        "\n",
        "        # Il tuo batch puÃ² restituire piÃ¹ elementi; qui estraiamo ciÃ² che serve\n",
        "        data, data1, targets, modelid, modelcat = batch\n",
        "\n",
        "        # Sposta su GPU (o sul device corretto) e converti in float se necessario\n",
        "        data = data.float().cuda()\n",
        "        targets = targets.float().cuda()\n",
        "\n",
        "        # Inferenza con il modello\n",
        "        afford_pred = model_instance(data)   # [batch_size, num_points, 2] ad es.\n",
        "\n",
        "        # Se la tua uscita Ã¨ di shape [B, N, 2], prendi l'argmax per ricavare la classe\n",
        "        afford_pred = torch.argmax(afford_pred, dim=-1)  # Shape: [1, 2048]\n",
        "\n",
        "        afford_pred = afford_pred ^ 1\n",
        "\n",
        "\n",
        "        new_targets = targets[:, :, 0]\n",
        "\n",
        "        # Convertilo su CPU per calcolo mIoU (se la tua funzione lo richiede)\n",
        "        afford_pred_cpu = afford_pred.detach().cpu().numpy()\n",
        "        ground_truth = new_targets.detach().cpu().numpy()  # [B, N]\n",
        "\n",
        "        # Calcolo del mIOU (supponendo che la tua funzione handle batch e shape correttamente)\n",
        "        miou = calculate_miou(afford_pred_cpu, ground_truth, threshold=0.05)\n",
        "        all_mious.append(miou)\n",
        "\n",
        "# Al termine, puoi calcolare la media delle metriche\n",
        "mean_miou = np.mean(all_mious)\n",
        "print(f\" IOU (val set): {all_mious}\")\n",
        "print(f\"Mean IOU (val set): {mean_miou}\")"
      ],
      "metadata": {
        "id": "9_88zHw1ffs6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4a5d04-2dbf-48c7-b9bc-fc80fb7c1966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " IOU (val set): [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Mean IOU (val set): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n"
      ],
      "metadata": {
        "id": "6gYkrgkCPG-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Â nuovo TEST\n",
        "\n",
        "\n",
        "model_instance.eval()  # imposta il modello in modalitÃ  valutazione\n",
        "\n",
        "all_mious = []\n",
        "\n",
        "with torch.no_grad():  # disabilita il calcolo dei gradienti per l'inferenza\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader[\"test_loader\"]):\n",
        "\n",
        "        if batch_idx == 1: # forbice strana\n",
        "         continue\n",
        "\n",
        "        # Il tuo batch puÃ² restituire piÃ¹ elementi; qui estraiamo ciÃ² che serve\n",
        "        data, data1, targets, modelid, modelcat = batch\n",
        "\n",
        "        # Sposta su GPU (o sul device corretto) e converti in float se necessario\n",
        "        data = data.float().cuda()\n",
        "        targets = targets.float().cuda()\n",
        "\n",
        "        # Inferenza con il modello\n",
        "        afford_pred = model_instance(data)   # [batch_size, num_points, 2] ad es.\n",
        "\n",
        "        # Se la tua uscita Ã¨ di shape [B, N, 2], prendi l'argmax per ricavare la classe\n",
        "        afford_pred = torch.argmax(afford_pred, dim=-1)  # Shape: [1, 2048]\n",
        "\n",
        "        afford_pred = afford_pred ^ 1\n",
        "\n",
        "\n",
        "        new_targets = targets[:, :, 0]\n",
        "\n",
        "        # Convertilo su CPU per calcolo mIoU (se la tua funzione lo richiede)\n",
        "        afford_pred_cpu = afford_pred.detach().cpu().numpy()\n",
        "        ground_truth = new_targets.detach().cpu().numpy()  # [B, N]\n",
        "\n",
        "        # Calcolo del mIOU (supponendo che la tua funzione handle batch e shape correttamente)\n",
        "        miou = calculate_miou(afford_pred_cpu, ground_truth, threshold=0.05)\n",
        "        all_mious.append(miou)\n",
        "\n",
        "# Al termine, puoi calcolare la media delle metriche\n",
        "mean_miou = np.mean(all_mious)\n",
        "print(f\" IOU (test set): {all_mious}\")\n",
        "print(f\"Mean IOU (test set): {mean_miou}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpLoEKOVPJB-",
        "outputId": "d56a6833-c9a9-4e28-870e-4113cd7e07aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " IOU (test set): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Mean IOU (test set): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extension"
      ],
      "metadata": {
        "id": "zRUrNNWeGOtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomApplySubset:\n",
        "    def __init__(self, transforms, n_select):\n",
        "\n",
        "        self.transforms = transforms\n",
        "        self.n_select = n_select\n",
        "\n",
        "    def __call__(self, image):\n",
        "\n",
        "        selected_transforms = random.sample(self.transforms, self.n_select)\n",
        "        for transform in selected_transforms:\n",
        "            #print(f\"Applying {transform} to image of type {type(image)}\")\n",
        "            image = transform(image)\n",
        "        return image\n",
        "\n",
        "\n",
        "class BackgroundTransform:\n",
        "    def __init__(self, backgrounds, blur_sigma=(0.1, 2.0)):\n",
        "\n",
        "        if not isinstance(backgrounds, list) or len(backgrounds) == 0:\n",
        "            raise ValueError(\"BackgroundTransform requires a non-empty list of background tensors.\")\n",
        "        if not all(isinstance(bg, torch.Tensor) for bg in backgrounds):\n",
        "            raise TypeError(\"All backgrounds must be torch tensors.\")\n",
        "\n",
        "        self.backgrounds = backgrounds\n",
        "        self.blur_sigma = blur_sigma\n",
        "\n",
        "    def __call__(self, rendered_images):\n",
        "\n",
        "        transformed_images = []\n",
        "\n",
        "        for rendered_image in rendered_images:\n",
        "            # Select a random background\n",
        "            bg = random.choice(self.backgrounds)\n",
        "            bg = bg.to(rendered_image.device)\n",
        "\n",
        "            # Resize the background to match the rendered image\n",
        "            bg = transforms.Resize(rendered_image.shape[-2:])(bg)\n",
        "\n",
        "            # Apply Gaussian blur to the background\n",
        "            blurred_bg = F.gaussian_blur(bg, kernel_size=(5, 5), sigma=random.uniform(*self.blur_sigma))\n",
        "\n",
        "            # Create a mask from the rendered image\n",
        "            mask = (rendered_image.sum(dim=0, keepdim=True) > 0).float()  # Mask identifies non-zero pixels\n",
        "\n",
        "            # Blend the rendered image with the blurred background\n",
        "            transformed_image = rendered_image * mask + blurred_bg * (1 - mask)\n",
        "\n",
        "            # Ensure output is valid\n",
        "            if transformed_image.ndim == 2:\n",
        "                transformed_image = transformed_image.unsqueeze(0)\n",
        "            transformed_image = transformed_image.to(rendered_image.device)\n",
        "\n",
        "            transformed_images.append(transformed_image)\n",
        "\n",
        "        transformed_images = torch.stack(transformed_images).to(rendered_images.device)\n",
        "\n",
        "        return transformed_images"
      ],
      "metadata": {
        "id": "MmdaR010IErD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Â extension su parte 3\n",
        "\n",
        "\n",
        "model_instance_ext = None\n",
        "\n",
        "learning_rate = 0.0001\n",
        "n_layers = 4\n",
        "n_views = 4\n",
        "n_augs = 3\n",
        "\n",
        "prompt= \"A 3D rendering of a pair of scissors, emphasizing the ergonomic handles, which are shaped for a firm and comfortable one-handed grip\"\n",
        "\n",
        "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "backgrounds = [\n",
        "    transforms.ToTensor()(Image.open(\"./data/bg1.jpg\").resize((224, 224))).to(device),\n",
        "    transforms.ToTensor()(Image.open(\"./data/bg2.jpg\").resize((224, 224))).to(device),\n",
        "    transforms.ToTensor()(Image.open(\"./data/bg3.jpg\").resize((224, 224))).to(device),\n",
        "    torch.tensor((1., 1., 1.)).to(device)\n",
        "]\n",
        "\n",
        "possible_augmentations = [\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5), #Â gia usata\n",
        "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2)) # new  introduce variazioni di rotazione, traslazione e scala\n",
        "\n",
        "]\n",
        "\n",
        "# Custom augmentation that randomly applies a subset\n",
        "augment_transform = transforms.Compose([\n",
        "    RandomApplySubset(possible_augmentations, n_select=2), #Â ora prova con 3\n",
        "    clip_normalizer\n",
        "])\n",
        "\n",
        "\n",
        "# Ottieni il primo batch dal val_loader\n",
        "first_batch = next(iter(loader[\"val_loader\"]))\n",
        "\n",
        "data, data1, targets, modelid, modelcat = first_batch\n",
        "vertex = data\n",
        "obj_path = 'alpha.obj'\n",
        "pointToMesh(data, obj_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_instance_ext = optimize(obj_path, learning_rate, n_layers, n_views, n_augs, prompt, augment_transform=augment_transform)\n"
      ],
      "metadata": {
        "id": "nKf_Z5-E8PIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validation"
      ],
      "metadata": {
        "id": "wP5z0sVOxSld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Â nuovo eval\n",
        "\n",
        "\n",
        "model_instance_ext.eval()  # imposta il modello in modalitÃ  valutazione\n",
        "\n",
        "all_mious = []\n",
        "\n",
        "with torch.no_grad():  # disabilita il calcolo dei gradienti per l'inferenza\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader[\"val_loader\"]):\n",
        "\n",
        "        if batch_idx == 1: #questo perchÃ¨ il primo valore lo uso per il train ( VA BENE ???????--- TODO ############)\n",
        "         continue\n",
        "\n",
        "        # Il tuo batch puÃ² restituire piÃ¹ elementi; qui estraiamo ciÃ² che serve\n",
        "        data, data1, targets, modelid, modelcat = batch\n",
        "\n",
        "        # Sposta su GPU (o sul device corretto) e converti in float se necessario\n",
        "        data = data.float().cuda()\n",
        "        targets = targets.float().cuda()\n",
        "\n",
        "        # Inferenza con il modello\n",
        "        afford_pred = model_instance_ext(data)   # [batch_size, num_points, 2] ad es.\n",
        "\n",
        "        # Se la tua uscita Ã¨ di shape [B, N, 2], prendi l'argmax per ricavare la classe\n",
        "        afford_pred = torch.argmax(afford_pred, dim=-1)  # Shape: [1, 2048]\n",
        "\n",
        "        afford_pred = afford_pred ^ 1\n",
        "\n",
        "\n",
        "        new_targets = targets[:, :, 0]\n",
        "\n",
        "        # Convertilo su CPU per calcolo mIoU (se la tua funzione lo richiede)\n",
        "        afford_pred_cpu = afford_pred.detach().cpu().numpy()\n",
        "        ground_truth = new_targets.detach().cpu().numpy()  # [B, N]\n",
        "\n",
        "        # Calcolo del mIOU (supponendo che la tua funzione handle batch e shape correttamente)\n",
        "        miou = calculate_miou(afford_pred_cpu, ground_truth, threshold=0.05)\n",
        "        all_mious.append(miou)\n",
        "\n",
        "# Al termine, puoi calcolare la media delle metriche\n",
        "mean_miou = np.mean(all_mious)\n",
        "print(f\" IOU (val set): {all_mious}\")\n",
        "print(f\"Mean IOU (val set): {mean_miou}\")"
      ],
      "metadata": {
        "id": "PV6C1DQYxUYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Â test\n"
      ],
      "metadata": {
        "id": "-4k015tOxg8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Â nuovo TEST\n",
        "\n",
        "\n",
        "model_instance_ext.eval()  # imposta il modello in modalitÃ  valutazione\n",
        "\n",
        "all_mious = []\n",
        "\n",
        "with torch.no_grad():  # disabilita il calcolo dei gradienti per l'inferenza\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader[\"test_loader\"]):\n",
        "\n",
        "        if batch_idx == 1: # forbice strana\n",
        "         continue\n",
        "\n",
        "        # Il tuo batch puÃ² restituire piÃ¹ elementi; qui estraiamo ciÃ² che serve\n",
        "        data, data1, targets, modelid, modelcat = batch\n",
        "\n",
        "        # Sposta su GPU (o sul device corretto) e converti in float se necessario\n",
        "        data = data.float().cuda()\n",
        "        targets = targets.float().cuda()\n",
        "\n",
        "        # Inferenza con il modello\n",
        "        afford_pred = model_instance_ext(data)   # [batch_size, num_points, 2] ad es.\n",
        "\n",
        "        # Se la tua uscita Ã¨ di shape [B, N, 2], prendi l'argmax per ricavare la classe\n",
        "        afford_pred = torch.argmax(afford_pred, dim=-1)  # Shape: [1, 2048]\n",
        "\n",
        "        afford_pred = afford_pred ^ 1\n",
        "\n",
        "\n",
        "        new_targets = targets[:, :, 0]\n",
        "\n",
        "        # Convertilo su CPU per calcolo mIoU (se la tua funzione lo richiede)\n",
        "        afford_pred_cpu = afford_pred.detach().cpu().numpy()\n",
        "        ground_truth = new_targets.detach().cpu().numpy()  # [B, N]\n",
        "\n",
        "        # Calcolo del mIOU (supponendo che la tua funzione handle batch e shape correttamente)\n",
        "        miou = calculate_miou(afford_pred_cpu, ground_truth, threshold=0.05)\n",
        "        all_mious.append(miou)\n",
        "\n",
        "# Al termine, puoi calcolare la media delle metriche\n",
        "mean_miou = np.mean(all_mious)\n",
        "print(f\" IOU (test set): {all_mious}\")\n",
        "print(f\"Mean IOU (test set): {mean_miou}\")"
      ],
      "metadata": {
        "id": "T4mi8BsoxiJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BwsC0Krr29MR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}