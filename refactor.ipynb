{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_Pgj7F7mKPYE",
        "AROS8JDQX2TL",
        "_iNaf_PSP-m8",
        "5Itwp0habDGF",
        "1s0WDrNHbJn4",
        "pRZcqd68bODB",
        "o04pJHaJbTSm",
        "uS2EPux5bmft",
        "idyg6lT5cCTa",
        "hK-nOIXNb1Y8",
        "ApCQHpy6cGm0",
        "O_AuisAogSUb"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Installation"
      ],
      "metadata": {
        "id": "_Pgj7F7mKPYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDA4Ch6WXJC9",
        "outputId": "18b4ccdd-c7df-4b7c-9c2f-d8ac9ff8d2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-e2wiikcy\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-e2wiikcy\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.8.61)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=9d2caf7084d9ffae148682c72666b102625cb250b4ad83ef7e0ebdbcd8675949\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2eo7umff/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n",
            "Looking in links: https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
            "Collecting kaolin==0.17.0\n",
            "  Downloading https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121/kaolin-0.17.0-cp311-cp311-linux_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipycanvas (from kaolin==0.17.0)\n",
            "  Downloading ipycanvas-0.13.3-py2.py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: ipyevents in /usr/local/lib/python3.11/dist-packages (from kaolin==0.17.0) (2.0.2)\n",
            "Requirement already satisfied: jupyter-client<8 in /usr/local/lib/python3.11/dist-packages (from kaolin==0.17.0) (6.1.12)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from kaolin==0.17.0) (3.1.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from kaolin==0.17.0) (6.3.3)\n",
            "Collecting comm>=0.1.3 (from kaolin==0.17.0)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting usd-core (from kaolin==0.17.0)\n",
            "  Downloading usd_core-25.2-cp311-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from kaolin==0.17.0) (1.26.4)\n",
            "Collecting pybind11 (from kaolin==0.17.0)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from kaolin==0.17.0) (11.1.0)\n",
            "Requirement already satisfied: tqdm>=4.51.0 in /usr/local/lib/python3.11/dist-packages (from kaolin==0.17.0) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from kaolin==0.17.0) (1.13.1)\n",
            "Collecting pygltflib (from kaolin==0.17.0)\n",
            "  Downloading pygltflib-1.16.3.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting warp-lang (from kaolin==0.17.0)\n",
            "  Downloading warp_lang-1.5.1-py3-none-manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from kaolin==0.17.0) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4 in /usr/local/lib/python3.11/dist-packages (from comm>=0.1.3->kaolin==0.17.0) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client<8->kaolin==0.17.0) (5.7.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client<8->kaolin==0.17.0) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client<8->kaolin==0.17.0) (2.8.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->kaolin==0.17.0) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask->kaolin==0.17.0) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->kaolin==0.17.0) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask->kaolin==0.17.0) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->kaolin==0.17.0) (1.9.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from ipycanvas->kaolin==0.17.0) (7.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->kaolin==0.17.0) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->kaolin==0.17.0)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->kaolin==0.17.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->kaolin==0.17.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->kaolin==0.17.0) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->kaolin==0.17.0) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->kaolin==0.17.0) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->kaolin==0.17.0) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->kaolin==0.17.0) (4.9.0)\n",
            "Collecting dataclasses-json>=0.0.25 (from pygltflib->kaolin==0.17.0)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.11/dist-packages (from pygltflib->kaolin==0.17.0) (1.2.17)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.0.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->kaolin==0.17.0) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask->kaolin==0.17.0) (3.0.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client<8->kaolin==0.17.0) (4.3.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->kaolin==0.17.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kaolin==0.17.0) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client<8->kaolin==0.17.0) (1.17.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated->pygltflib->kaolin==0.17.0) (1.17.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (24.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (4.12.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (6.5.5)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (7.16.5)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.2.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.1.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.22.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.3.1)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading ipycanvas-0.13.3-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading usd_core-25.2-cp311-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.6/25.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading warp_lang-1.5.1-py3-none-manylinux2014_x86_64.whl (84.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: pygltflib\n",
            "  Building wheel for pygltflib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygltflib: filename=pygltflib-1.16.3-py3-none-any.whl size=27408 sha256=c89f407fe5f270a57bf14c4cf1d57e9b690531f9e7a0190342211517a7ece482\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/75/a7/661db79759f1de86850bbbc3112020f4d18436bad637ea5673\n",
            "Successfully built pygltflib\n",
            "Installing collected packages: warp-lang, usd-core, pybind11, mypy-extensions, marshmallow, jedi, comm, typing-inspect, dataclasses-json, pygltflib, ipycanvas, kaolin\n",
            "Successfully installed comm-0.2.2 dataclasses-json-0.6.7 ipycanvas-0.13.3 jedi-0.19.2 kaolin-0.17.0 marshmallow-3.26.0 mypy-extensions-1.0.0 pybind11-2.13.6 pygltflib-1.16.3 typing-inspect-0.9.0 usd-core-25.2 warp-lang-1.5.1\n",
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-2.30.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.20.1+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.27.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.2)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.0->open-clip-torch) (12.8.61)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2024.12.14)\n",
            "Downloading open_clip_torch-2.30.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: open-clip-torch\n",
            "Successfully installed open-clip-torch-2.30.0\n",
            "Collecting open3d\n",
            "  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.26.4)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\n",
            "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.0)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.1.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting flask>=3.0.0 (from open3d)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting werkzeug>=3.0.0 (from open3d)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.1.0)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.55.6)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2024.12.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, addict, widgetsnbextension, werkzeug, retrying, pyquaternion, configargparse, flask, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 3.1.0\n",
            "    Uninstalling Flask-3.1.0:\n",
            "      Successfully uninstalled Flask-3.1.0\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 configargparse-1.7 dash-2.18.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 flask-3.0.3 ipywidgets-8.1.5 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.13\n",
            "Cloning into 'AML_group'...\n",
            "remote: Enumerating objects: 241, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 241 (delta 31), reused 12 (delta 3), pack-reused 187 (from 1)\u001b[K\n",
            "Receiving objects: 100% (241/241), 11.10 MiB | 18.25 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n",
            "/content/AML_group\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
        "!pip install open-clip-torch\n",
        "\n",
        "!pip install open3d\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"AML_group\"):\n",
        "    !git clone https://github.com/AlesCarl/AML_group.git\n",
        "else:\n",
        "    print(\"Repository già clonato.\")\n",
        "%cd AML_group\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "6LyyUROpKMQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import open_clip\n",
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "from itertools import permutations, product\n",
        "from Normalization import MeshNormalizer\n",
        "from utils import device, color_mesh\n",
        "from mesh import Mesh\n",
        "from render import Renderer\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SaK9SPJ9XLtJ",
        "outputId": "50df9baa-f724-46f3-83df-07aaf3f8b8e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warp 1.5.1 initialized:\n",
            "   CUDA Toolkit 12.6, Driver 12.2\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Highlighter Class"
      ],
      "metadata": {
        "id": "AROS8JDQX2TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm([hidden_dim]))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "        self.mlp = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.mlp:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xZft-SeZXzYD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "ZdoZUiilX-Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clip_model(clip_model):\n",
        "    device = 'cuda'\n",
        "    model, preprocess = clip.load(clip_model, device=device) # jit = True for better perfomance\n",
        "    return model\n",
        "\n",
        "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5, show=False, center_azim=0, center_elev=0, std=1, return_views=True, lighting=True, background=background)\n",
        "\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "def clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "    if n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if encoded_text.shape[0] > 1:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "        else:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "    elif n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            #print(augmented_image.shape)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "        loss= loss / n_augs\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
      ],
      "metadata": {
        "id": "zGgxV8xkYBvS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer Loop"
      ],
      "metadata": {
        "id": "PzXksQQ4YQoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(obj_path, learning_rate, n_layers, n_views, n_augs, prompt, augment_transform=None):\n",
        "    render_res = 224\n",
        "    n_iter = 1000\n",
        "    res = 224\n",
        "    output_dir = './output/'\n",
        "    clip_model = 'ViT-L/14'\n",
        "\n",
        "    input_dim = 3\n",
        "    hidden_dim = 256\n",
        "    output_dim = 2\n",
        "\n",
        "    Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "    #render = Renderer(dim=(render_res, render_res), background_image= None )\n",
        "    render = Renderer(dim=(render_res, render_res), background_image='./data/desk1.jpg')\n",
        "\n",
        "    mesh = Mesh(obj_path)\n",
        "    MeshNormalizer(mesh)()\n",
        "\n",
        "    # Initialize variables\n",
        "    background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "    # CLIP and augmentation transform\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    if augment_transform is None:\n",
        "        augment_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "            transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "            clip_normalizer\n",
        "        ])\n",
        "\n",
        "    # MLP and optimizer Settings\n",
        "    mlp = NeuralHighlighter(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "    # List of possible colors\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "    # Encode prompt with CLIP\n",
        "    model = get_clip_model(clip_model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    vertices = copy.deepcopy(mesh.vertices)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in tqdm(range(n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        sampled_mesh = mesh\n",
        "        color_mesh(pred_class, sampled_mesh, colors)\n",
        "        rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views, show=False, center_azim=0, center_elev=0, std=1, return_views=True, lighting=True, background=background) #,background=background)\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, model)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(output_dir, i, rendered_images)\n",
        "            with open(os.path.join(output_dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "\n",
        "    # save results\n",
        "    save_final_results(output_dir, 'final_mesh', mesh, mlp, vertices, colors, render, background) ##\n",
        "\n",
        "    return mlp"
      ],
      "metadata": {
        "id": "XvdLVtA1YTUa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Part"
      ],
      "metadata": {
        "id": "_iNaf_PSP-m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_path = 'data/horse.obj'\n",
        "\n",
        "learning_rate = 0.0001\n",
        "n_layers = 5\n",
        "n_views = 4\n",
        "n_augs = 4\n",
        "\n",
        "prompt= \"A 3D rendering of a Horse with highlighted Shoes.\"\n",
        "\n",
        "model_instance = optimize(obj_path, learning_rate, n_layers, n_views, n_augs, prompt)"
      ],
      "metadata": {
        "id": "hW6byH5pI_4O",
        "outputId": "961c47b2-e2a7-43b0-d784-3c49897a6e9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Background image path:  ./data/bg1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|████████                               | 185M/890M [00:05<00:20, 36.1MiB/s]\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Part"
      ],
      "metadata": {
        "id": "5Itwp0habDGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mesh 2 Point Cloud"
      ],
      "metadata": {
        "id": "1s0WDrNHbJn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso del file OBJ\n",
        "obj_path = 'data/horse.obj'\n",
        "\n",
        "# Funzione per caricare la mesh come TriangleMesh\n",
        "def load_obj_as_triangle_mesh(obj_path):\n",
        "    # Carica il file .obj\n",
        "    mesh = o3d.io.read_triangle_mesh(obj_path)\n",
        "    if mesh.is_empty():\n",
        "        raise ValueError(f\"La mesh nel file {obj_path} non è stata caricata correttamente.\")\n",
        "    return mesh\n",
        "\n",
        "# Carica la mesh\n",
        "mesh = load_obj_as_triangle_mesh(obj_path)\n",
        "\n",
        "# Converte la mesh in una point cloud campionando punti uniformemente\n",
        "pcd = mesh.sample_points_uniformly(2048)\n",
        "\n",
        "# Imposta il colore nero per tutti i punti\n",
        "pcd.colors = o3d.utility.Vector3dVector(np.zeros((len(pcd.points), 3)))  # Colore nero: [0, 0, 0]\n",
        "\n",
        "# Esporta la point cloud in formato PLY\n",
        "# o3d.io.write_point_cloud(\"candle_PC.ply\", pcd)\n",
        "\n",
        "# Point cloud to mesh\n",
        "\n",
        "pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
        "\n",
        "with o3d.utility.VerbosityContextManager(\n",
        "        o3d.utility.VerbosityLevel.Debug) as cm:\n",
        "    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n",
        "        pcd, depth=9)\n",
        "print(mesh)\n",
        "\n",
        "o3d.io.write_triangle_mesh(\"APPROXIMATE.obj\", mesh)"
      ],
      "metadata": {
        "id": "nfV8ll1ibFGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63279fd4-1dd1-477d-ec28-2b9d84b3c9ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Open3D DEBUG] Input Points / Samples: 2048 / 2046\n",
            "[Open3D DEBUG] #   Got kernel density: 0.01088404655456543 (s), 3936.60546875 (MB) / 3936.60546875 (MB) / 4028 (MB)\n",
            "[Open3D DEBUG] #     Got normal field: 0.008194923400878906 (s), 3937.4296875 (MB) / 3937.4296875 (MB) / 4028 (MB)\n",
            "[Open3D DEBUG] Point weight / Estimated Area: 5.753411e-04 / 1.178298e+00\n",
            "[Open3D DEBUG] #       Finalized tree: 0.02439403533935547 (s), 3937.6875 (MB) / 3937.6875 (MB) / 4028 (MB)\n",
            "[Open3D DEBUG] #  Set FEM constraints: 0.15444207191467285 (s), 3937.81640625 (MB) / 3937.81640625 (MB) / 4028 (MB)\n",
            "[Open3D DEBUG] #Set point constraints: 0.007436990737915039 (s), 3937.81640625 (MB) / 3937.81640625 (MB) / 4028 (MB)\n",
            "[Open3D DEBUG] Leaf Nodes / Active Nodes / Ghost Nodes: 153931 / 88248 / 87673\n",
            "[Open3D DEBUG] Memory Usage: 3937.816 MB\n",
            "[Open3D DEBUG] # Linear system solved: 0.13541483879089355 (s), 3937.8984375 (MB) / 3937.8984375 (MB) / 4028 (MB)\n",
            "[Open3D DEBUG] Got average: 0.005606889724731445 (s), 3937.8984375 (MB) / 3937.8984375 (MB) / 4028 (MB)\n",
            "[Open3D DEBUG] Iso-Value: 5.052341e-01 = 1.034719e+03 / 2.048000e+03\n",
            "[Open3D DEBUG] #          Total Solve:       0.9 (s),    3939.0 (MB)\n",
            "TriangleMesh with 5898 points and 11792 triangles.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Point Cloud 2 Mesh"
      ],
      "metadata": {
        "id": "pRZcqd68bODB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pointToMesh(data, output_path):\n",
        "    single_data = data[0].cpu().numpy()\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(single_data)\n",
        "\n",
        "    # Stima delle normali\n",
        "    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
        "        radius=0.06,  # Raggio per la stima delle normali,\n",
        "        max_nn=80     # Numero massimo di vicini per stimare la normale\n",
        "    ))\n",
        "\n",
        "    # Orienta le normali in modo consistente\n",
        "    pcd.orient_normals_consistent_tangent_plane(k=100)\n",
        "\n",
        "    # Ricostruzione con\n",
        "    mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_alpha_shape(\n",
        "        pcd, 0.04\n",
        "    )\n",
        "\n",
        "    # Salva e visualizza la mesh\n",
        "    o3d.io.write_triangle_mesh(output_path, mesh)"
      ],
      "metadata": {
        "id": "58Kjfs3nbHvS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third Part"
      ],
      "metadata": {
        "id": "o04pJHaJbTSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AffordanceNet Class to handle the dataset"
      ],
      "metadata": {
        "id": "uS2EPux5bmft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pc_normalize(pc):\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc, centroid, m\n",
        "\n",
        "\n",
        "class AffordNetDataset(Dataset):\n",
        "    def __init__(self, data_dir, split):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "\n",
        "        self.load_data()\n",
        "        self.affordance = self.all_data[0][\"affordance\"]\n",
        "        return\n",
        "\n",
        "    def load_data(self):\n",
        "     self.all_data = []\n",
        "     with open(os.path.join(self.data_dir, 'full_shape_%s_data.pkl' % self.split), 'rb') as f:\n",
        "        temp_data = pkl.load(f)\n",
        "     for index, info in enumerate(temp_data):\n",
        "        if info[\"semantic class\"] == \"Scissors\":  # Filtra solo gli oggetti \"BOWL\"\n",
        "            temp_info = {}\n",
        "            temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "            temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "            temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "            temp_info[\"data_info\"] = info[\"full_shape\"] # vertici\n",
        "            self.all_data.append(temp_info)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_dict = self.all_data[index]\n",
        "        modelid = data_dict[\"shape_id\"]\n",
        "        modelcat = data_dict[\"semantic class\"]\n",
        "\n",
        "        data_info = data_dict[\"data_info\"]\n",
        "        model_data = data_info[\"coordinate\"].astype(np.float32)\n",
        "        labels = data_info[\"label\"]\n",
        "        for aff in self.affordance:\n",
        "            temp = labels[aff].astype(np.float32).reshape(-1, 1)\n",
        "            model_data = np.concatenate((model_data, temp), axis=1)\n",
        "\n",
        "        datas = model_data[:, :3]\n",
        "        targets = model_data[:, 3:]\n",
        "        datas, _, _ = pc_normalize(datas)\n",
        "\n",
        "        return datas, datas, targets, modelid, modelcat\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_random_sample(self):\n",
        "        index = random.randint(0, len(self.all_data) - 1)\n",
        "        return self.__getitem__(index)\n",
        "\n",
        "\n",
        "\n",
        "def build_dataset(data_dir, test=False):\n",
        "    test_set = AffordNetDataset(data_dir, 'train')\n",
        "    val_set = AffordNetDataset(data_dir, 'val')\n",
        "\n",
        "    # Seleziona i primi 5 campioni per il VAL set\n",
        "    ### val_indices = list(range(min(6, len(val_set)))) # uno in più\n",
        "    val_indices = random.sample(range(len(val_set)), min(6, len(val_set))) # casuali\n",
        "\n",
        "    val_set = Subset(val_set, val_indices)\n",
        "\n",
        "    # Seleziona 10 campioni per il TEST set, a partire dal 7° (indice 6)\n",
        "    test_start_index = 6\n",
        "    test_indices = list(range(test_start_index, test_start_index + min(10, len(test_set) - test_start_index)))\n",
        "    test_set = Subset(test_set, test_indices)\n",
        "\n",
        "    # Ritorna i dataset in un dizionario\n",
        "    dataset_dict = dict(val_set=val_set, test_set=test_set)\n",
        "    return dataset_dict\n",
        "\n",
        "def build_loader(dataset_dict):\n",
        "    val_set = dataset_dict[\"val_set\"]\n",
        "    test_set = dataset_dict[\"test_set\"]\n",
        "\n",
        "    batch_size_factor = 1\n",
        "\n",
        "    test_loader = DataLoader(test_set, batch_size=1, shuffle=True, drop_last=False, num_workers=8)\n",
        "    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=8, drop_last=False)\n",
        "    #random_sample = val_set.dataset.get_random_sample() # new\n",
        "\n",
        "\n",
        "    loader_dict = dict(val_loader=val_loader, test_loader=test_loader) # new\n",
        "    return loader_dict"
      ],
      "metadata": {
        "id": "fZax0_YKbU6D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the mIOU"
      ],
      "metadata": {
        "id": "idyg6lT5cCTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_miou(predictions, ground_truths, threshold):\n",
        "    ground_truths = (ground_truths >= threshold).astype(int)  # Binarizza ground truth\n",
        "\n",
        "    batch_size, num_points = ground_truths.shape\n",
        "    iou_per_class = np.zeros((batch_size, 1))\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        pred = predictions[b, :]\n",
        "        gt = ground_truths[b, :]\n",
        "\n",
        "        # Calcola intersezione e unione\n",
        "        intersection = np.sum(pred * gt)\n",
        "        union = np.sum(pred + gt) - intersection\n",
        "\n",
        "        if union == 0:  # Evita divisione per zero\n",
        "            iou_per_class[b, 0] = np.nan  # Non valido se non ci sono punti\n",
        "        else:\n",
        "            iou_per_class[b, 0] = intersection / union\n",
        "\n",
        "    # Media su batch e classi\n",
        "    mean_iou = np.nanmean(iou_per_class)\n",
        "    return mean_iou"
      ],
      "metadata": {
        "id": "H2IN3MbpcEQ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset"
      ],
      "metadata": {
        "id": "hK-nOIXNb1Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_dir = '/content/drive/My Drive/full-shape'\n",
        "dataset_dir = '/content/drive/My Drive/ColabNotebooks'\n",
        "\n",
        "dataset = build_dataset(dataset_dir)\n",
        "\n",
        "loader = build_loader(dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "NYcuPRP3b2ym",
        "outputId": "72eb72ed-f837-4fe6-d7d0-7f65c2381b65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "ApCQHpy6cGm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training + Validation\n"
      ],
      "metadata": {
        "id": "PDjXAm4yPDKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_instance = None\n",
        "\n",
        "\n",
        "learning_rate = 0.0001\n",
        "n_layers = 4\n",
        "n_views = 4\n",
        "n_augs = 3\n",
        "\n",
        "\n",
        "\n",
        "#prompt= \"A 3D rendering of a pair of scissors, emphasizing the handles, which are shaped for a one-handed grip\" # training di una sola\n",
        "prompt= \"A 3D rendering of a pair of scissors, emphasizing the ergonomic handles, which are shaped for a firm and comfortable one-handed grip\"\n",
        "\n",
        "\n",
        "\n",
        "# Ottieni il primo batch dal val_loader\n",
        "first_batch = next(iter(loader[\"val_loader\"]))\n",
        "\n",
        "data, data1, targets, modelid, modelcat = first_batch\n",
        "vertex = data\n",
        "obj_path = 'alpha.obj'\n",
        "pointToMesh(data, obj_path)\n",
        "\n",
        "model_instance = optimize(obj_path, learning_rate, n_layers, n_views, n_augs, prompt) # 1 solo training\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WJPW6K4XcIBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81114ce-3f9d-4403-d82b-db501f58e426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Background image path:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1000 [00:00<11:09,  1.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.2412109375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 101/1000 [01:04<09:44,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.257255859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 201/1000 [02:09<08:41,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24457275390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 301/1000 [03:14<07:38,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.248316650390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 401/1000 [04:20<06:37,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24991943359375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 501/1000 [05:25<05:25,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.249744873046875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 601/1000 [06:30<04:21,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24816162109375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 701/1000 [07:36<03:15,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24906982421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 801/1000 [08:41<02:10,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.250093994140625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 901/1000 [09:46<01:04,  1.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.24994384765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [10:52<00:00,  1.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nuovo eval\n",
        "\n",
        "\n",
        "model_instance.eval()  # imposta il modello in modalità valutazione\n",
        "\n",
        "all_mious = []\n",
        "\n",
        "with torch.no_grad():  # disabilita il calcolo dei gradienti per l'inferenza\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader[\"val_loader\"]):\n",
        "\n",
        "        if batch_idx == 1: #questo perchè il primo valore lo uso per il train ( VA BENE ???????--- TODO ############)\n",
        "         continue\n",
        "\n",
        "        # Il tuo batch può restituire più elementi; qui estraiamo ciò che serve\n",
        "        data, data1, targets, modelid, modelcat = batch\n",
        "\n",
        "        # Sposta su GPU (o sul device corretto) e converti in float se necessario\n",
        "        data = data.float().cuda()\n",
        "        targets = targets.float().cuda()\n",
        "\n",
        "        # Inferenza con il modello\n",
        "        afford_pred = model_instance(data)   # [batch_size, num_points, 2] ad es.\n",
        "\n",
        "        # Se la tua uscita è di shape [B, N, 2], prendi l'argmax per ricavare la classe\n",
        "        afford_pred = torch.argmax(afford_pred, dim=-1)  # Shape: [1, 2048]\n",
        "\n",
        "        afford_pred = afford_pred ^ 1\n",
        "\n",
        "\n",
        "        new_targets = targets[:, :, 0]\n",
        "\n",
        "        # Convertilo su CPU per calcolo mIoU (se la tua funzione lo richiede)\n",
        "        afford_pred_cpu = afford_pred.detach().cpu().numpy()\n",
        "        ground_truth = new_targets.detach().cpu().numpy()  # [B, N]\n",
        "\n",
        "        # Calcolo del mIOU (supponendo che la tua funzione handle batch e shape correttamente)\n",
        "        miou = calculate_miou(afford_pred_cpu, ground_truth, threshold=0.05)\n",
        "        all_mious.append(miou)\n",
        "\n",
        "# Al termine, puoi calcolare la media delle metriche\n",
        "mean_miou = np.mean(all_mious)\n",
        "print(f\" IOU (val set): {all_mious}\")\n",
        "print(f\"Mean IOU (val set): {mean_miou}\")"
      ],
      "metadata": {
        "id": "9_88zHw1ffs6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4a5d04-2dbf-48c7-b9bc-fc80fb7c1966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " IOU (val set): [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Mean IOU (val set): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n"
      ],
      "metadata": {
        "id": "6gYkrgkCPG-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nuovo TEST\n",
        "\n",
        "\n",
        "model_instance.eval()  # imposta il modello in modalità valutazione\n",
        "\n",
        "all_mious = []\n",
        "\n",
        "with torch.no_grad():  # disabilita il calcolo dei gradienti per l'inferenza\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader[\"test_loader\"]):\n",
        "\n",
        "        if batch_idx == 1: # forbice strana\n",
        "         continue\n",
        "\n",
        "        # Il tuo batch può restituire più elementi; qui estraiamo ciò che serve\n",
        "        data, data1, targets, modelid, modelcat = batch\n",
        "\n",
        "        # Sposta su GPU (o sul device corretto) e converti in float se necessario\n",
        "        data = data.float().cuda()\n",
        "        targets = targets.float().cuda()\n",
        "\n",
        "        # Inferenza con il modello\n",
        "        afford_pred = model_instance(data)   # [batch_size, num_points, 2] ad es.\n",
        "\n",
        "        # Se la tua uscita è di shape [B, N, 2], prendi l'argmax per ricavare la classe\n",
        "        afford_pred = torch.argmax(afford_pred, dim=-1)  # Shape: [1, 2048]\n",
        "\n",
        "        afford_pred = afford_pred ^ 1\n",
        "\n",
        "\n",
        "        new_targets = targets[:, :, 0]\n",
        "\n",
        "        # Convertilo su CPU per calcolo mIoU (se la tua funzione lo richiede)\n",
        "        afford_pred_cpu = afford_pred.detach().cpu().numpy()\n",
        "        ground_truth = new_targets.detach().cpu().numpy()  # [B, N]\n",
        "\n",
        "        # Calcolo del mIOU (supponendo che la tua funzione handle batch e shape correttamente)\n",
        "        miou = calculate_miou(afford_pred_cpu, ground_truth, threshold=0.05)\n",
        "        all_mious.append(miou)\n",
        "\n",
        "# Al termine, puoi calcolare la media delle metriche\n",
        "mean_miou = np.mean(all_mious)\n",
        "print(f\" IOU (test set): {all_mious}\")\n",
        "print(f\"Mean IOU (test set): {mean_miou}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpLoEKOVPJB-",
        "outputId": "d56a6833-c9a9-4e28-870e-4113cd7e07aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " IOU (test set): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Mean IOU (test set): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extension"
      ],
      "metadata": {
        "id": "zRUrNNWeGOtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomApplySubset:\n",
        "    def __init__(self, transforms, n_select):\n",
        "        \"\"\"\n",
        "        Initialize with a list of transformations and the number of transformations to apply.\n",
        "        :param transforms: List of possible transformations.\n",
        "        :param n_select: Number of transformations to randomly select and apply.\n",
        "        \"\"\"\n",
        "        self.transforms = transforms\n",
        "        self.n_select = n_select\n",
        "\n",
        "    def __call__(self, image):\n",
        "        \"\"\"\n",
        "        Randomly selects a subset of transformations and applies them to the image.\n",
        "        :param image: Input image to transform.\n",
        "        :return: Transformed image.\n",
        "        \"\"\"\n",
        "        selected_transforms = random.sample(self.transforms, self.n_select)\n",
        "        for transform in selected_transforms:\n",
        "            #print(f\"Applying {transform} to image of type {type(image)}\")\n",
        "            image = transform(image)\n",
        "        return image\n",
        "\n",
        "\n",
        "class BackgroundTransform:\n",
        "    def __init__(self, backgrounds, blur_sigma=(0.1, 2.0)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            backgrounds (list of torch.Tensor): List of background images as tensors.\n",
        "            blur_sigma (tuple): Min and max sigma for Gaussian blur.\n",
        "        \"\"\"\n",
        "        if not isinstance(backgrounds, list) or len(backgrounds) == 0:\n",
        "            raise ValueError(\"BackgroundTransform requires a non-empty list of background tensors.\")\n",
        "        if not all(isinstance(bg, torch.Tensor) for bg in backgrounds):\n",
        "            raise TypeError(\"All backgrounds must be torch tensors.\")\n",
        "\n",
        "        self.backgrounds = backgrounds\n",
        "        self.blur_sigma = blur_sigma\n",
        "\n",
        "    def __call__(self, rendered_images):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rendered_images (torch.Tensor)[]: Input images tensor of shape (B, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Transformed image blended with a random background.\n",
        "        \"\"\"\n",
        "        transformed_images = []\n",
        "\n",
        "        for rendered_image in rendered_images:\n",
        "            # Select a random background\n",
        "            bg = random.choice(self.backgrounds)\n",
        "            bg = bg.to(rendered_image.device)\n",
        "\n",
        "            # Resize the background to match the rendered image\n",
        "            bg = transforms.Resize(rendered_image.shape[-2:])(bg)\n",
        "\n",
        "            # Apply Gaussian blur to the background\n",
        "            blurred_bg = F.gaussian_blur(bg, kernel_size=(5, 5), sigma=random.uniform(*self.blur_sigma))\n",
        "\n",
        "            # Create a mask from the rendered image\n",
        "            mask = (rendered_image.sum(dim=0, keepdim=True) > 0).float()  # Mask identifies non-zero pixels\n",
        "\n",
        "            # Blend the rendered image with the blurred background\n",
        "            transformed_image = rendered_image * mask + blurred_bg * (1 - mask)\n",
        "\n",
        "            # Ensure output is valid\n",
        "            if transformed_image.ndim == 2:\n",
        "                transformed_image = transformed_image.unsqueeze(0)\n",
        "            transformed_image = transformed_image.to(rendered_image.device)\n",
        "\n",
        "            transformed_images.append(transformed_image)\n",
        "\n",
        "        transformed_images = torch.stack(transformed_images).to(rendered_images.device)\n",
        "\n",
        "        return transformed_images"
      ],
      "metadata": {
        "id": "MmdaR010IErD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extension su parte 3\n",
        "\n",
        "\n",
        "model_instance_ext = None\n",
        "\n",
        "learning_rate = 0.0001\n",
        "n_layers = 4\n",
        "n_views = 4\n",
        "n_augs = 3\n",
        "\n",
        "prompt= \"A 3D rendering of a pair of scissors, emphasizing the ergonomic handles, which are shaped for a firm and comfortable one-handed grip\"\n",
        "\n",
        "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "backgrounds = [\n",
        "    transforms.ToTensor()(Image.open(\"./data/bg1.jpg\").resize((224, 224))).to(device),\n",
        "    transforms.ToTensor()(Image.open(\"./data/bg2.jpg\").resize((224, 224))).to(device),\n",
        "    transforms.ToTensor()(Image.open(\"./data/bg3.jpg\").resize((224, 224))).to(device),\n",
        "    torch.tensor((1., 1., 1.)).to(device)\n",
        "]\n",
        "\n",
        "possible_augmentations = [\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5), # gia usata\n",
        "    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2)) # new  introduce variazioni di rotazione, traslazione e scala\n",
        "\n",
        "]\n",
        "\n",
        "# Custom augmentation that randomly applies a subset\n",
        "augment_transform = transforms.Compose([\n",
        "    RandomApplySubset(possible_augmentations, n_select=3),\n",
        "    clip_normalizer\n",
        "])\n",
        "\n",
        "\n",
        "# Ottieni il primo batch dal val_loader\n",
        "first_batch = next(iter(loader[\"val_loader\"]))\n",
        "\n",
        "data, data1, targets, modelid, modelcat = first_batch\n",
        "vertex = data\n",
        "obj_path = 'alpha.obj'\n",
        "pointToMesh(data, obj_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_instance_ext = optimize(obj_path, learning_rate, n_layers, n_views, n_augs, prompt, augment_transform=augment_transform)\n"
      ],
      "metadata": {
        "id": "nKf_Z5-E8PIC",
        "outputId": "e9b4f0d7-6960-463a-f160-cee07f5e424f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Background image path:  ./data/desk1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1000 [00:00<05:46,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.27001953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 101/1000 [00:29<04:32,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.288148193359375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 201/1000 [00:59<03:52,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.283773193359375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 301/1000 [01:28<03:30,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.2856298828125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 401/1000 [01:57<02:51,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.283837890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 501/1000 [02:26<02:22,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.282958984375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 601/1000 [02:55<01:53,  3.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.284599609375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 701/1000 [03:24<01:25,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.285047607421875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 801/1000 [03:53<00:56,  3.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.281114501953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 901/1000 [04:22<00:28,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.2847900390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:52<00:00,  3.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validation"
      ],
      "metadata": {
        "id": "wP5z0sVOxSld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nuovo eval\n",
        "\n",
        "\n",
        "model_instance_ext.eval()  # imposta il modello in modalità valutazione\n",
        "\n",
        "all_mious = []\n",
        "\n",
        "with torch.no_grad():  # disabilita il calcolo dei gradienti per l'inferenza\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader[\"val_loader\"]):\n",
        "\n",
        "        if batch_idx == 1: #questo perchè il primo valore lo uso per il train ( VA BENE ???????--- TODO ############)\n",
        "         continue\n",
        "\n",
        "        # Il tuo batch può restituire più elementi; qui estraiamo ciò che serve\n",
        "        data, data1, targets, modelid, modelcat = batch\n",
        "\n",
        "        # Sposta su GPU (o sul device corretto) e converti in float se necessario\n",
        "        data = data.float().cuda()\n",
        "        targets = targets.float().cuda()\n",
        "\n",
        "        # Inferenza con il modello\n",
        "        afford_pred = model_instance_ext(data)   # [batch_size, num_points, 2] ad es.\n",
        "\n",
        "        # Se la tua uscita è di shape [B, N, 2], prendi l'argmax per ricavare la classe\n",
        "        afford_pred = torch.argmax(afford_pred, dim=-1)  # Shape: [1, 2048]\n",
        "\n",
        "        afford_pred = afford_pred ^ 1\n",
        "\n",
        "\n",
        "        new_targets = targets[:, :, 0]\n",
        "\n",
        "        # Convertilo su CPU per calcolo mIoU (se la tua funzione lo richiede)\n",
        "        afford_pred_cpu = afford_pred.detach().cpu().numpy()\n",
        "        ground_truth = new_targets.detach().cpu().numpy()  # [B, N]\n",
        "\n",
        "        # Calcolo del mIOU (supponendo che la tua funzione handle batch e shape correttamente)\n",
        "        miou = calculate_miou(afford_pred_cpu, ground_truth, threshold=0.05)\n",
        "        all_mious.append(miou)\n",
        "\n",
        "# Al termine, puoi calcolare la media delle metriche\n",
        "mean_miou = np.mean(all_mious)\n",
        "print(f\" IOU (val set): {all_mious}\")\n",
        "print(f\"Mean IOU (val set): {mean_miou}\")"
      ],
      "metadata": {
        "id": "PV6C1DQYxUYS",
        "outputId": "b1ed308b-615a-4eb5-8c98-d544bb9a1898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " IOU (val set): [0.8603712671509282, 0.7659574468085106, 0.4767267267267267, 0.44312267657992566, 0.4876237623762376]\n",
            "Mean IOU (val set): 0.6067603759284657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test\n"
      ],
      "metadata": {
        "id": "-4k015tOxg8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nuovo TEST\n",
        "\n",
        "\n",
        "model_instance_ext.eval()  # imposta il modello in modalità valutazione\n",
        "\n",
        "all_mious = []\n",
        "\n",
        "with torch.no_grad():  # disabilita il calcolo dei gradienti per l'inferenza\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader[\"test_loader\"]):\n",
        "\n",
        "        if batch_idx == 1: # forbice strana\n",
        "         continue\n",
        "\n",
        "        # Il tuo batch può restituire più elementi; qui estraiamo ciò che serve\n",
        "        data, data1, targets, modelid, modelcat = batch\n",
        "\n",
        "        # Sposta su GPU (o sul device corretto) e converti in float se necessario\n",
        "        data = data.float().cuda()\n",
        "        targets = targets.float().cuda()\n",
        "\n",
        "        # Inferenza con il modello\n",
        "        afford_pred = model_instance_ext(data)   # [batch_size, num_points, 2] ad es.\n",
        "\n",
        "        # Se la tua uscita è di shape [B, N, 2], prendi l'argmax per ricavare la classe\n",
        "        afford_pred = torch.argmax(afford_pred, dim=-1)  # Shape: [1, 2048]\n",
        "\n",
        "        afford_pred = afford_pred ^ 1\n",
        "\n",
        "\n",
        "        new_targets = targets[:, :, 0]\n",
        "\n",
        "        # Convertilo su CPU per calcolo mIoU (se la tua funzione lo richiede)\n",
        "        afford_pred_cpu = afford_pred.detach().cpu().numpy()\n",
        "        ground_truth = new_targets.detach().cpu().numpy()  # [B, N]\n",
        "\n",
        "        # Calcolo del mIOU (supponendo che la tua funzione handle batch e shape correttamente)\n",
        "        miou = calculate_miou(afford_pred_cpu, ground_truth, threshold=0.05)\n",
        "        all_mious.append(miou)\n",
        "\n",
        "# Al termine, puoi calcolare la media delle metriche\n",
        "mean_miou = np.mean(all_mious)\n",
        "print(f\" IOU (test set): {all_mious}\")\n",
        "print(f\"Mean IOU (test set): {mean_miou}\")"
      ],
      "metadata": {
        "id": "T4mi8BsoxiJi",
        "outputId": "067236fc-569b-4bde-bd74-fb2592e11f63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " IOU (test set): [0.9263233190271817, 0.9569413511507052, 0.5978181818181818, 0.6489122280570142, 0.7271329746348962, 0.724429416737109, 0.8079937304075235, 0.7477477477477478, 0.9151785714285714]\n",
            "Mean IOU (test set): 0.7836086134454366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BwsC0Krr29MR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}