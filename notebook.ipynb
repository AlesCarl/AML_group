{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c9Im4BMmdSi",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
        "!pip install open-clip-torch\n",
        "\n",
        "\n",
        "!git clone https://github.com/AlesCarl/AML_group.git\n",
        "%cd AML_group\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-tLaxIg3mdSj",
        "outputId": "a80f1189-44ec-4eb8-b5ec-907c713b8a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warp 1.5.1 initialized:\n",
            "   CUDA Toolkit 12.6, Driver 12.2\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.5.1\n"
          ]
        }
      ],
      "source": [
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import open_clip\n",
        "\n",
        "\n",
        "from itertools import permutations, product\n",
        "from Normalization import MeshNormalizer\n",
        "\n",
        "\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "\n",
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm([hidden_dim]))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "        self.mlp = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.mlp:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_clip_model(clip_model):\n",
        "    device = 'cuda'\n",
        "    model, preprocess = clip.load(clip_model, device=device) # jit = True for better perfomance\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
        "                                                                        show=False,\n",
        "                                                                        center_azim=0,\n",
        "                                                                        center_elev=0,\n",
        "                                                                        std=1,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "\n",
        "    if n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if encoded_text.shape[0] > 1:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "        else:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "    elif n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "    return loss\n",
        "    # \"1-loss\" removed -> now best value in output is -1\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "53FxBU81mdSj",
        "outputId": "881df01f-cdfb-40a8-aea1-b113b49bbd55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/clip/clip.py:57: UserWarning: /root/.cache/clip/ViT-L-14.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
            "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
            "100%|████████████████████████████████████████| 890M/890M [00:08<00:00, 105MiB/s]\n",
            "  0%|          | 1/1000 [00:02<33:27,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.2398681640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 101/1000 [00:22<03:03,  4.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.30088134765625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 201/1000 [00:43<02:46,  4.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.30932373046875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 301/1000 [01:04<02:29,  4.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.31841064453125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 401/1000 [01:25<02:07,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.32242431640625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 501/1000 [01:47<01:47,  4.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.3262255859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 601/1000 [02:08<01:28,  4.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.32611328125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 701/1000 [02:30<01:05,  4.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.3264404296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 801/1000 [02:52<00:43,  4.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.3250048828125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 901/1000 [03:13<00:21,  4.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.321964111328125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [03:35<00:00,  4.64it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected str, bytes or os.PathLike object, not builtin_function_or_method",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-41d15dd4d74c>\u001b[0m in \u001b[0;36m<cell line: 127>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# Save prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdiscarded\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     ends with a separator.\"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not builtin_function_or_method"
          ]
        }
      ],
      "source": [
        "# Constrain most sources of randomness\n",
        "# (some torch backwards functions within CLIP are non-determinstic)\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "render_res = 224\n",
        "n_iter = 1000\n",
        "res = 224\n",
        "obj_path = 'data/horse.obj' # 'APPROXIMATE.obj' if we want to test the point cloud to mesh conversion\n",
        "output_dir = './output/'\n",
        "clip_model = 'ViT-L/14'\n",
        "\n",
        "input_dim = 3\n",
        "hidden_dim = 256\n",
        "output_dim = 2\n",
        "\n",
        "# Hyper-parameters\n",
        "learning_rate = 0.0001\n",
        "n_layers = 3 # depth 4\n",
        "n_views = 3 # 5\n",
        "n_augs = 1 # 4\n",
        "\n",
        "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "render = Renderer(dim=(render_res, render_res))\n",
        "mesh = Mesh(obj_path)\n",
        "MeshNormalizer(mesh)()\n",
        "\n",
        "# Initialize variables\n",
        "background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "log_dir = output_dir\n",
        "\n",
        "\n",
        "# CLIP and augmentation transform\n",
        "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "clip_transform = transforms.Compose([\n",
        "    transforms.Resize((res, res)),\n",
        "    clip_normalizer\n",
        "])\n",
        "augment_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "    clip_normalizer\n",
        "])\n",
        "\n",
        "\n",
        "# MLP Settings\n",
        "mlp = NeuralHighlighter(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "# list of possible colors\n",
        "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "# --- Prompt ---\n",
        "# encode prompt with CLIP\n",
        "model = get_clip_model(clip_model)\n",
        "\n",
        "known_object = 'horse'\n",
        "classes = 'Shoes'\n",
        "\n",
        "prompt = \"A 3D render of a gray {} with highlighted {}\".format(known_object, classes)\n",
        "with torch.no_grad():\n",
        "    prompt_token = clip.tokenize([prompt]).to(device)\n",
        "    encoded_text = model.encode_text(prompt_token)\n",
        "    encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "vertices = copy.deepcopy(mesh.vertices)\n",
        "\n",
        "losses = []\n",
        "\n",
        "# Optimization loop\n",
        "for i in tqdm(range(n_iter)):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # predict highlight probabilities\n",
        "    pred_class = mlp(vertices)\n",
        "\n",
        "    # color and render mesh\n",
        "    sampled_mesh = mesh\n",
        "    color_mesh(pred_class, sampled_mesh, colors)\n",
        "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                            show=False,\n",
        "                                                            center_azim=0,\n",
        "                                                            center_elev=0,\n",
        "                                                            std=1,\n",
        "                                                            return_views=True,\n",
        "                                                            lighting=True,\n",
        "                                                            background=background)\n",
        "\n",
        "    # Calculate CLIP Loss\n",
        "    loss = clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, model)\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    # update variables + record loss\n",
        "    with torch.no_grad():\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # report results\n",
        "    if i % 100 == 0:\n",
        "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "        save_renders(log_dir, i, rendered_images)\n",
        "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
        "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "\n",
        "# save results\n",
        "save_final_results(log_dir, 'Primo test', mesh, mlp, vertices, colors, render, background)\n",
        "\n",
        "# Save prompts\n",
        "with open(os.path.join(dir, prompt), \"w\") as f:\n",
        "    f.write('')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P9tTHppoKMeq",
        "outputId": "2ed99739-dbb8-4c08-9f75-a465ee97bd26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mesh to Point Cloud using open3d\n",
        "\n",
        "!pip install open3d\n",
        "\n",
        "import open3d as o3d\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Percorso del file OBJ\n",
        "obj_path = 'data/candle.obj'\n",
        "\n",
        "# Funzione per caricare la mesh come TriangleMesh\n",
        "def load_obj_as_triangle_mesh(obj_path):\n",
        "    # Carica il file .obj\n",
        "    mesh = o3d.io.read_triangle_mesh(obj_path)\n",
        "    if mesh.is_empty():\n",
        "        raise ValueError(f\"La mesh nel file {obj_path} non è stata caricata correttamente.\")\n",
        "    return mesh\n",
        "\n",
        "# Carica la mesh\n",
        "mesh = load_obj_as_triangle_mesh(obj_path)\n",
        "\n",
        "# Converte la mesh in una point cloud campionando punti uniformemente\n",
        "pcd = mesh.sample_points_uniformly(10000)\n",
        "\n",
        "# Imposta il colore nero per tutti i punti\n",
        "pcd.colors = o3d.utility.Vector3dVector(np.zeros((len(pcd.points), 3)))  # Colore nero: [0, 0, 0]\n",
        "\n",
        "\n",
        "# Esporta la point cloud in formato PLY\n",
        "# o3d.io.write_point_cloud(\"candle_PC.ply\", pcd)"
      ],
      "metadata": {
        "id": "kFSUF3vsa1a2",
        "outputId": "187dee47-8496-4202-80b7-5ebb0b431981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: open3d in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.26.4)\n",
            "Requirement already satisfied: dash>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.18.2)\n",
            "Requirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.6)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\n",
            "Requirement already satisfied: configargparse in /usr/local/lib/python3.10/dist-packages (from open3d) (1.7)\n",
            "Requirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from open3d) (8.1.5)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from open3d) (2.4.0)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (11.0.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.8.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.67.1)\n",
            "Requirement already satisfied: pyquaternion in /usr/local/lib/python3.10/dist-packages (from open3d) (0.9.9)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (3.0.3)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (75.1.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (3.0.2)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (1.9.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2024.12.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "HtJalqZNHcmE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Point cloud to mesh\n",
        "\n",
        "pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
        "\n",
        "with o3d.utility.VerbosityContextManager(\n",
        "        o3d.utility.VerbosityLevel.Debug) as cm:\n",
        "    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n",
        "        pcd, depth=9)\n",
        "print(mesh)\n",
        "\n",
        "o3d.io.write_triangle_mesh(\"APPROXIMATE.obj\", mesh)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transforms3d\n",
        "\n",
        "\n",
        "from transforms3d.quaternions import quat2mat\n",
        "from transforms3d.euler import euler2mat\n",
        "\n",
        "\n",
        "\n",
        "def random_rotation_matrix():\n",
        "    rand = np.random.rand(3)\n",
        "    r1 = np.sqrt(1.0 - rand[0])\n",
        "    r2 = np.sqrt(rand[0])\n",
        "    pi2 = np.pi * 2.0\n",
        "    t1 = pi2 * rand[1]\n",
        "    t2 = pi2 * rand[2]\n",
        "    q = np.array([np.cos(t2)*r2, np.sin(t1)*r1, np.cos(t1)*r1, np.sin(t2)*r2])\n",
        "    return quat2mat(q)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rotate_point_cloud_SO3(batch_data):\n",
        "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
        "    for k in range(batch_data.shape[0]):\n",
        "        rotation_matrix = random_rotation_matrix()\n",
        "        shape_pc = batch_data[k, ...]\n",
        "        rotated_data[k, ...] = (\n",
        "            np.matmul(rotation_matrix, shape_pc.reshape((-1, 3)).T)).T\n",
        "    return rotated_data\n",
        "\n",
        "\n",
        "def rotate_point_cloud_y(batch_data):\n",
        "\n",
        "    rotated_data = np.zeros(batch_data.shape, dtype=np.float32)\n",
        "    for k in range(batch_data.shape[0]):\n",
        "        rotation_angle = np.random.uniform() * 2 * np.pi\n",
        "        rotation_matrix = euler2mat(0, rotation_angle, 0)\n",
        "        shape_pc = batch_data[k, ...]\n",
        "        rotated_data[k, ...] = (\n",
        "            np.matmul(rotation_matrix, shape_pc.reshape((-1, 3)).T)).T\n",
        "    return rotated_data"
      ],
      "metadata": {
        "id": "k9szcsLJcSU-",
        "outputId": "ff940491-0629-4753-a369-d3768665664d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transforms3d\n",
            "  Downloading transforms3d-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from transforms3d) (1.26.4)\n",
            "Downloading transforms3d-0.4.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transforms3d\n",
            "Successfully installed transforms3d-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import join as opj\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import json\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "def pc_normalize(pc):\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc, centroid, m\n",
        "\n",
        "\n",
        "class AffordNetDataset(Dataset):\n",
        "    def __init__(self, data_dir, split):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "        self.affordance = self.all_data[0][\"affordance\"]\n",
        "\n",
        "        return\n",
        "\n",
        "    def load_data(self):\n",
        "        self.all_data = []\n",
        "        with open(opj(self.data_dir, 'full_shape_%s_data.pkl' % self.split), 'rb') as f:\n",
        "                    temp_data = pkl.load(f)\n",
        "        for index, info in enumerate(temp_data):\n",
        "              temp_info = {}\n",
        "              temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "              temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "              temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "              temp_info[\"data_info\"] = info[\"full_shape\"]\n",
        "              self.all_data.append(temp_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        data_dict = self.all_data[index]\n",
        "        modelid = data_dict[\"shape_id\"]\n",
        "        modelcat = data_dict[\"semantic class\"]\n",
        "\n",
        "        data_info = data_dict[\"data_info\"]\n",
        "        model_data = data_info[\"coordinate\"].astype(np.float32)\n",
        "        labels = data_info[\"label\"]\n",
        "        for aff in self.affordance:\n",
        "            temp = labels[aff].astype(np.float32).reshape(-1, 1)\n",
        "            model_data = np.concatenate((model_data, temp), axis=1)\n",
        "\n",
        "        datas = model_data[:, :3]\n",
        "        targets = model_data[:, 3:]\n",
        "\n",
        "        datas, _, _ = pc_normalize(datas)\n",
        "\n",
        "        return datas, datas, targets, modelid, modelcat\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)"
      ],
      "metadata": {
        "id": "GMWezA4kbbvl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def build_dataset(data_dir, test=False):\n",
        "    train_set = AffordNetDataset(\n",
        "            data_dir, 'train')\n",
        "    val_set = AffordNetDataset(\n",
        "            data_dir, 'val')\n",
        "    dataset_dict = dict(train_set=train_set, val_set=val_set)\n",
        "    return dataset_dict\n",
        "\n",
        "\n",
        "def build_loader(dataset_dict):\n",
        "    train_set = dataset_dict[\"train_set\"]\n",
        "    val_set = dataset_dict[\"val_set\"]\n",
        "    batch_size_factor = 1\n",
        "    train_loader = DataLoader(train_set, batch_size=cfg.training_cfg.batch_size,\n",
        "                              shuffle=True, drop_last=True, num_workers=8)\n",
        "    val_loader = DataLoader(val_set, batch_size=1,\n",
        "                            shuffle=False, num_workers=8, drop_last=False)\n",
        "    loader_dict = dict(\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader\n",
        "    )\n",
        "\n",
        "    return loader_dict"
      ],
      "metadata": {
        "id": "RaLjnrIucF8x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from os.path import join as opj\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import average_precision_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "def evaluation(model, test_loader, affordance):\n",
        "    results = torch.zeros(\n",
        "        (len(test_loader), 2048, len(affordance)))\n",
        "    targets = torch.zeros(\n",
        "        (len(test_loader), 2048, len(affordance)))\n",
        "    coordinate = np.zeros((0, 2048, 3))\n",
        "    modelids = []\n",
        "    modelcats = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_L2distance = 0\n",
        "        count = 0.0\n",
        "        for i,  temp_data in tqdm(enumerate(test_loader), total=len(test_loader), smoothing=0.9):\n",
        "\n",
        "            # modelcat identifies the category of the 3d model we are passing to our model\n",
        "            (data, data1, label, modelid, modelcat) = temp_data\n",
        "\n",
        "            data, label = data.float().cuda(), label.float().cuda()\n",
        "            data = data.permute(0, 2, 1)\n",
        "            batch_size = data.size()[0]\n",
        "            num_point = data.size()[2]\n",
        "            count += batch_size * num_point\n",
        "            afford_pred = torch.sigmoid(model(data))\n",
        "            afford_pred = afford_pred.permute(0, 2, 1).contiguous()\n",
        "            L2distance = torch.sum(\n",
        "                torch.pow(label-afford_pred, 2), dim=(0, 1))\n",
        "            total_L2distance += L2distance\n",
        "            score = afford_pred.squeeze()\n",
        "            target_score = label.squeeze()\n",
        "            results[i, :, :] = score\n",
        "            targets[i, :, :] = target_score\n",
        "            modelids.append(modelid[0])\n",
        "            modelcats.append(modelcat[0])\n",
        "\n",
        "    results = results.detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    IOU = np.zeros((targets.shape[0], targets.shape[2]))\n",
        "    targets = targets >= 0.5\n",
        "    targets = targets.astype(int)\n",
        "\n",
        "    IOU_thres = np.linspace(0, 1, 20)\n",
        "    for i in range(IOU.shape[0]):\n",
        "        t = targets[i, :, :]\n",
        "        p = results[i, :, :]\n",
        "        for j in range(t.shape[1]):\n",
        "            t_true = t[:, j]\n",
        "            p_score = p[:, j]\n",
        "            if np.sum(t_true) == 0:\n",
        "                IOU[i, j] = np.nan\n",
        "            else:\n",
        "                p_mask = (p_score > 0.5).astype(int)\n",
        "                temp_iou = []\n",
        "                for thre in IOU_thres:\n",
        "                    p_mask = (p_score >= thre).astype(int)\n",
        "                    intersect = np.sum(p_mask & t_true)\n",
        "                    union = np.sum(p_mask | t_true)\n",
        "                    temp_iou.append(1.*intersect/union)\n",
        "                temp_iou = np.array(temp_iou)\n",
        "                aiou = np.mean(temp_iou)\n",
        "                IOU[i, j] = aiou\n",
        "\n",
        "    IOU = np.nanmean(IOU, axis=0)\n",
        "\n",
        "    outstr = 'Test :: test maIOU: %.6f' % (\n",
        "        np.mean(IOU))\n",
        "\n",
        "    return np.mean(IOU)"
      ],
      "metadata": {
        "id": "b7bndVjRaA5d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = './data'\n",
        "\n",
        "dataset = build_dataset(dataset_dir)\n",
        "loader = build_loader(dataset)\n",
        "\n",
        "mIOU = evaluation(mlp, loader, ['Grasp', 'Push', 'Wrap'])\n",
        "print(mIOU)"
      ],
      "metadata": {
        "id": "48VXxFvQdSjc",
        "outputId": "87a86fab-b841-4e74-c646-e758b84a45e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/full_shape_train_data.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f228388f31b6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-49a65c3d68dd>\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(data_dir, test)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     train_set = AffordNetDataset(\n\u001b[0m\u001b[1;32m      5\u001b[0m             data_dir, 'train')\n\u001b[1;32m      6\u001b[0m     val_set = AffordNetDataset(\n",
            "\u001b[0;32m<ipython-input-12-a17fcaf1f5bd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, split)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffordance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"affordance\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a17fcaf1f5bd>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'full_shape_%s_data.pkl'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0mtemp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/full_shape_train_data.pkl'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}