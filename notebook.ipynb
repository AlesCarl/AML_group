{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:46:17.413176Z",
     "start_time": "2024-12-15T14:46:02.823127Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c9Im4BMmdSi",
    "outputId": "6b576111-8ef3-4b21-edf7-5dfacd020112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\r\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/db/psm35sh503x7y51vnw94qbzw0000gn/T/pip-req-build-1hghit7a\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/db/psm35sh503x7y51vnw94qbzw0000gn/T/pip-req-build-1hghit7a\r\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\r\n",
      "  Installing build dependencies ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\r\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from clip==1.0) (6.3.1)\r\n",
      "Requirement already satisfied: packaging in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from clip==1.0) (24.2)\r\n",
      "Requirement already satisfied: regex in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from clip==1.0) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from clip==1.0) (4.67.1)\r\n",
      "Requirement already satisfied: torch in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from clip==1.0) (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from clip==1.0) (0.17.2)\r\n",
      "Requirement already satisfied: wcwidth in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from ftfy->clip==1.0) (0.2.13)\r\n",
      "Requirement already satisfied: filelock in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from torch->clip==1.0) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from torch->clip==1.0) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from torch->clip==1.0) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from torch->clip==1.0) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from torch->clip==1.0) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from torch->clip==1.0) (2024.10.0)\r\n",
      "Requirement already satisfied: numpy in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from torchvision->clip==1.0) (2.2.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from torchvision->clip==1.0) (11.0.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/Frutto/.pyenv/versions/3.12.6/envs/aml_group/lib/python3.12/site-packages (from sympy->torch->clip==1.0) (1.3.0)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Looking in links: https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\r\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement kaolin==0.17.0 (from versions: 0.1)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for kaolin==0.17.0\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "###!pip install kaolin -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.0_cu111.html\n",
    "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
    "\n",
    "\n",
    "# !git clone https://github.com/AlesCarl/AML_group.git\n",
    "##!ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-tLaxIg3mdSj"
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from itertools import permutations, product\n",
    "from Normalization import MeshNormalizer\n",
    "\n",
    "\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from utils import device, color_mesh\n",
    "\n",
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm([hidden_dim]))\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.mlp = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def get_clip_model(clipmodel):\n",
    "    raise NotImplementedError(\"Load the clip model from the clip module\")\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=1,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "\n",
    "def clip_loss():\n",
    "\n",
    "    raise NotImplementedError(\"\\\n",
    "    Implement the Neural Highlighter Model as described in the project instructions \\\n",
    "    Pass to this function the language embedding, the rendered images and the clip model\\\n",
    "    return the calculated loss     \\\n",
    "    \")\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53FxBU81mdSj"
   },
   "outputs": [],
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "render_res = 224\n",
    "learning_rate = 0.0001\n",
    "n_iter = 2500\n",
    "res = 224\n",
    "obj_path = 'data/horse.obj'\n",
    "n_augs = 5\n",
    "output_dir = './output/'\n",
    "clip_model = 'ViT-L/14'\n",
    "\n",
    "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "# Initialize variables\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "\n",
    "log_dir = output_dir\n",
    "\n",
    "\n",
    "# MLP Settings\n",
    "mlp = NeuralHighlighter().to(device)\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "# list of possible colors\n",
    "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
    "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
    "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
    "colors = torch.tensor(full_colors).to(device)\n",
    "\n",
    "\n",
    "# --- Prompt ---\n",
    "# encode prompt with CLIP\n",
    "get_clip_model(clip_model)\n",
    "prompt = ''\n",
    "\n",
    "\n",
    "vertices = copy.deepcopy(mesh.vertices)\n",
    "n_views = 5\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Optimization loop\n",
    "for i in tqdm(range(n_iter)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # predict highlight probabilities\n",
    "    pred_class = mlp(vertices)\n",
    "\n",
    "    # color and render mesh\n",
    "    sampled_mesh = mesh\n",
    "    color_mesh(pred_class, sampled_mesh, colors)\n",
    "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
    "                                                            show=False,\n",
    "                                                            center_azim=0,\n",
    "                                                            center_elev=0,\n",
    "                                                            std=1,\n",
    "                                                            return_views=True,\n",
    "                                                            lighting=True,\n",
    "                                                            background=background)\n",
    "\n",
    "    # Calculate CLIP Loss\n",
    "    loss = clip_loss()\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    # update variables + record loss\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # report results\n",
    "    if i % 100 == 0:\n",
    "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
    "        save_renders(log_dir, i, rendered_images)\n",
    "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
    "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
    "\n",
    "\n",
    "# save results\n",
    "save_final_results(log_dir, mesh, mlp, vertices, colors, render, background)\n",
    "\n",
    "# Save prompts\n",
    "with open(os.path.join(dir, prompt), \"w\") as f:\n",
    "    f.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_pytorch3d=False\n",
    "try:\n",
    "    import pytorch3d\n",
    "except ModuleNotFoundError:\n",
    "    need_pytorch3d=True\n",
    "if need_pytorch3d:\n",
    "    if torch.__version__.startswith(\"2.2.\") and sys.platform.startswith(\"linux\"):\n",
    "        # We try to install PyTorch3D via a released wheel.\n",
    "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "        version_str=\"\".join([\n",
    "            f\"py3{sys.version_info.minor}_cu\",\n",
    "            torch.version.cuda.replace(\".\",\"\"),\n",
    "            f\"_pyt{pyt_version_str}\"\n",
    "        ])\n",
    "        %pip install fvcore iopath\n",
    "        %pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
    "    else:\n",
    "        # We try to install PyTorch3D from source.\n",
    "        %pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aml_group",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
