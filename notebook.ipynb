{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "8c9Im4BMmdSi",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f691a3cf-d568-4222-859d-dd84a214f0bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-wyhrv0ne\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-wyhrv0ne\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Looking in links: https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
            "Requirement already satisfied: kaolin==0.17.0 in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: ipycanvas in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (0.13.3)\n",
            "Requirement already satisfied: ipyevents in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (2.0.2)\n",
            "Requirement already satisfied: jupyter-client<8 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (6.1.12)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (3.0.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (6.3.3)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (0.2.2)\n",
            "Requirement already satisfied: usd-core in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (24.11)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (1.26.4)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (2.13.6)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (11.1.0)\n",
            "Requirement already satisfied: tqdm>=4.51.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (1.13.1)\n",
            "Requirement already satisfied: pygltflib in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (1.16.3)\n",
            "Requirement already satisfied: warp-lang in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (1.5.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4 in /usr/local/lib/python3.10/dist-packages (from comm>=0.1.3->kaolin==0.17.0) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (5.7.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (2.8.2)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (3.0.6)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (1.9.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.10/dist-packages (from ipycanvas->kaolin==0.17.0) (8.1.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (4.9.0)\n",
            "Requirement already satisfied: dataclasses-json>=0.0.25 in /usr/local/lib/python3.10/dist-packages (from pygltflib->kaolin==0.17.0) (0.6.7)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from pygltflib->kaolin==0.17.0) (1.2.15)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (3.25.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (0.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.0.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->kaolin==0.17.0) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask->kaolin==0.17.0) (3.0.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client<8->kaolin==0.17.0) (4.3.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->kaolin==0.17.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kaolin==0.17.0) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client<8->kaolin==0.17.0) (1.17.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pygltflib->kaolin==0.17.0) (1.17.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (24.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (4.12.2)\n",
            "Requirement already satisfied: open-clip-torch in /usr/local/lib/python3.10/dist-packages (2.30.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.20.1+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.27.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.5.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (1.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2024.12.14)\n",
            "Requirement already satisfied: open3d in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.26.4)\n",
            "Requirement already satisfied: dash>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.18.2)\n",
            "Requirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.6)\n",
            "Requirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.3)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\n",
            "Requirement already satisfied: configargparse in /usr/local/lib/python3.10/dist-packages (from open3d) (1.7)\n",
            "Requirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.10/dist-packages (from open3d) (8.1.5)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from open3d) (2.4.0)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (11.1.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.67.1)\n",
            "Requirement already satisfied: pyquaternion in /usr/local/lib/python3.10/dist-packages (from open3d) (0.9.9)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (75.1.0)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=3.0.0->open3d) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask>=3.0.0->open3d) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2024.12.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Cloning into 'AML_group'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 128 (delta 61), reused 49 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (128/128), 2.77 MiB | 22.19 MiB/s, done.\n",
            "Resolving deltas: 100% (61/61), done.\n",
            "/content/AML_group\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
        "!pip install open-clip-torch\n",
        "\n",
        "!pip install open3d\n",
        "\n",
        "\n",
        "if not os.path.exists(\"AML_group\"):\n",
        "    !git clone https://github.com/AlesCarl/AML_group.git\n",
        "else:\n",
        "    print(\"Repository già clonato.\")\n",
        "%cd AML_group\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "-tLaxIg3mdSj"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import open_clip\n",
        "import open3d as o3d\n",
        "\n",
        "\n",
        "from itertools import permutations, product\n",
        "from Normalization import MeshNormalizer\n",
        "\n",
        "\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "\n",
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm([hidden_dim]))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "        layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "        self.mlp = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.mlp:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_clip_model(clip_model):\n",
        "    device = 'cuda'\n",
        "    model, preprocess = clip.load(clip_model, device=device) # jit = True for better perfomance\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
        "                                                                        show=False,\n",
        "                                                                        center_azim=0,\n",
        "                                                                        center_elev=0,\n",
        "                                                                        std=1,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "\n",
        "    if n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if encoded_text.shape[0] > 1:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "        else:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "    elif n_augs > 0:\n",
        "        loss = 0.0\n",
        "        for _ in range(n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "    return loss\n",
        "    # \"1-loss\" removed -> now best value in output is -1\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53FxBU81mdSj"
      },
      "outputs": [],
      "source": [
        "# Constrain most sources of randomness\n",
        "# (some torch backwards functions within CLIP are non-determinstic)\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "render_res = 224\n",
        "n_iter = 1000\n",
        "res = 224\n",
        "obj_path = 'data/horse.obj' # 'APPROXIMATE.obj' if we want to test the point cloud to mesh conversion\n",
        "output_dir = './output/'\n",
        "clip_model = 'ViT-L/14'\n",
        "\n",
        "input_dim = 3\n",
        "hidden_dim = 256\n",
        "output_dim = 2\n",
        "\n",
        "# Hyper-parameters\n",
        "learning_rate = 0.0001\n",
        "n_layers = 3 # depth 4\n",
        "n_views = 5 # 5\n",
        "n_augs = 1 # 4\n",
        "\n",
        "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "render = Renderer(dim=(render_res, render_res))\n",
        "mesh = Mesh(obj_path)\n",
        "MeshNormalizer(mesh)()\n",
        "\n",
        "\n",
        "### --\n",
        "\n",
        "# Initialize variables\n",
        "background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "log_dir = output_dir\n",
        "\n",
        "\n",
        "# CLIP and augmentation transform\n",
        "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "clip_transform = transforms.Compose([\n",
        "    transforms.Resize((res, res)),\n",
        "    clip_normalizer\n",
        "])\n",
        "augment_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "    transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "    clip_normalizer\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# MLP Settings\n",
        "mlp = NeuralHighlighter(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "# list of possible colors\n",
        "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "# --- Prompt ---\n",
        "# encode prompt with CLIP\n",
        "model = get_clip_model(clip_model)\n",
        "\n",
        "known_object = 'horse'\n",
        "classes = 'Shoes'\n",
        "\n",
        "\n",
        "prompt = \"A 3D render of a gray {} with highlighted {}\".format(known_object, classes)\n",
        "with torch.no_grad():\n",
        "    prompt_token = clip.tokenize([prompt]).to(device)\n",
        "    encoded_text = model.encode_text(prompt_token)\n",
        "    encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "vertices = copy.deepcopy(mesh.vertices)\n",
        "\n",
        "losses = []\n",
        "\n",
        "# Optimization loop\n",
        "for i in tqdm(range(n_iter)):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # predict highlight probabilities\n",
        "    pred_class = mlp(vertices)\n",
        "\n",
        "    # color and render mesh\n",
        "    sampled_mesh = mesh\n",
        "    color_mesh(pred_class, sampled_mesh, colors)\n",
        "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                            show=False,\n",
        "                                                            center_azim=0,\n",
        "                                                            center_elev=0,\n",
        "                                                            std=1,\n",
        "                                                            return_views=True,\n",
        "                                                            lighting=True,\n",
        "                                                            background=background)\n",
        "\n",
        "    # Calculate CLIP Loss\n",
        "    loss = clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, model)\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    # update variables + record loss\n",
        "    with torch.no_grad():\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # report results\n",
        "    if i % 100 == 0:\n",
        "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "        save_renders(log_dir, i, rendered_images)\n",
        "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
        "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "\n",
        "# save results\n",
        "save_final_results(log_dir, 'Primo test', mesh, mlp, vertices, colors, render, background)\n",
        "\n",
        "# Save prompts\n",
        "with open(os.path.join(dir, prompt), \"w\") as f:\n",
        "    f.write('')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P9tTHppoKMeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb2d7e3-250c-45d8-d7df-46fc0b343646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Percorso del file OBJ\n",
        "obj_path = 'data/horse.obj'\n",
        "\n",
        "# Funzione per caricare la mesh come TriangleMesh\n",
        "def load_obj_as_triangle_mesh(obj_path):\n",
        "    # Carica il file .obj\n",
        "    mesh = o3d.io.read_triangle_mesh(obj_path)\n",
        "    if mesh.is_empty():\n",
        "        raise ValueError(f\"La mesh nel file {obj_path} non è stata caricata correttamente.\")\n",
        "    return mesh\n",
        "\n",
        "# Carica la mesh\n",
        "mesh = load_obj_as_triangle_mesh(obj_path)\n",
        "\n",
        "# Converte la mesh in una point cloud campionando punti uniformemente\n",
        "pcd = mesh.sample_points_uniformly(2048)\n",
        "\n",
        "# Imposta il colore nero per tutti i punti\n",
        "pcd.colors = o3d.utility.Vector3dVector(np.zeros((len(pcd.points), 3)))  # Colore nero: [0, 0, 0]\n",
        "\n",
        "\n",
        "# Esporta la point cloud in formato PLY\n",
        "# o3d.io.write_point_cloud(\"candle_PC.ply\", pcd)"
      ],
      "metadata": {
        "id": "kFSUF3vsa1a2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HtJalqZNHcmE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Point cloud to mesh\n",
        "\n",
        "pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
        "\n",
        "with o3d.utility.VerbosityContextManager(\n",
        "        o3d.utility.VerbosityLevel.Debug) as cm:\n",
        "    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n",
        "        pcd, depth=9)\n",
        "print(mesh)\n",
        "\n",
        "o3d.io.write_triangle_mesh(\"APPROXIMATE.obj\", mesh)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 part"
      ],
      "metadata": {
        "id": "qqRzSRxrq-F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import join as opj\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import json\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "def pc_normalize(pc):\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc, centroid, m\n",
        "\n",
        "\n",
        "class AffordNetDataset(Dataset):\n",
        "    def __init__(self, data_dir, split):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "        self.affordance = self.all_data[0][\"affordance\"]\n",
        "\n",
        "        return\n",
        "\n",
        "    def load_data(self):\n",
        "     self.all_data = []\n",
        "     with open(opj(self.data_dir, 'full_shape_%s_data.pkl' % self.split), 'rb') as f:\n",
        "        temp_data = pkl.load(f)\n",
        "     for index, info in enumerate(temp_data):\n",
        "        if info[\"semantic class\"] == \"Bowl\":  # Filtra solo gli oggetti \"BOWL\"\n",
        "            temp_info = {}\n",
        "            temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "            temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "            temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "            temp_info[\"data_info\"] = info[\"full_shape\"] # vertici\n",
        "            self.all_data.append(temp_info)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        data_dict = self.all_data[index]\n",
        "        modelid = data_dict[\"shape_id\"]\n",
        "        modelcat = data_dict[\"semantic class\"]\n",
        "\n",
        "        data_info = data_dict[\"data_info\"]\n",
        "        model_data = data_info[\"coordinate\"].astype(np.float32)\n",
        "        labels = data_info[\"label\"]\n",
        "        for aff in self.affordance:\n",
        "            temp = labels[aff].astype(np.float32).reshape(-1, 1)\n",
        "            model_data = np.concatenate((model_data, temp), axis=1)\n",
        "\n",
        "        datas = model_data[:, :3]\n",
        "        targets = model_data[:, 3:]\n",
        "\n",
        "        datas, _, _ = pc_normalize(datas)\n",
        "\n",
        "        return datas, datas, targets, modelid, modelcat\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)"
      ],
      "metadata": {
        "id": "GMWezA4kbbvl"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "def build_dataset(data_dir, test=False):\n",
        "    test_set = AffordNetDataset( data_dir, 'train')\n",
        "    val_set = AffordNetDataset( data_dir, 'val')\n",
        "\n",
        "     # 5 campioni per il VAL set\n",
        "    val_indices = list(range(min(5, len(val_set))))\n",
        "    val_set = Subset(val_set, val_indices)\n",
        "\n",
        "    # 10 campioni per il test set\n",
        "    test_indices = list(range(min(10, len(test_set))))\n",
        "    test_set = Subset(test_set, test_indices)\n",
        "\n",
        "    dataset_dict = dict( val_set=val_set, test_set=test_set)\n",
        "    return dataset_dict\n",
        "\n",
        "\n",
        "def build_loader(dataset_dict):\n",
        "    val_set = dataset_dict[\"val_set\"]\n",
        "    test_set = dataset_dict[\"test_set\"]\n",
        "\n",
        "    batch_size_factor = 1\n",
        "\n",
        "\n",
        "    test_loader = DataLoader(test_set, batch_size=8, ## occhio qui\n",
        "                              shuffle=True, drop_last=True, num_workers=8)\n",
        "\n",
        "    val_loader = DataLoader(val_set, batch_size=1,\n",
        "                            shuffle=False, num_workers=8, drop_last=False)\n",
        "    loader_dict = dict(\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader ,\n",
        "    )\n",
        "\n",
        "    return loader_dict"
      ],
      "metadata": {
        "id": "RaLjnrIucF8x"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_dir = '/content/drive/My Drive/full-shape'\n",
        "dataset_dir = '/content/drive/My Drive/ColabNotebooks'\n",
        "\n",
        "dataset = build_dataset(dataset_dir)\n",
        "\n",
        "loader = build_loader(dataset)"
      ],
      "metadata": {
        "id": "48VXxFvQdSjc"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pointToMesh(data ):\n",
        "\n",
        "    single_data = data[0].cpu().numpy()     # shape [N, 3]\n",
        "    single_target = targets[0].cpu().numpy() # shape [N, num_affordances].     PER COSA LO USA\n",
        "\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(single_data)\n",
        "\n",
        "    # Stima delle normali\n",
        "    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
        "        radius=0.08,  # Raggio per la stima delle normali, aumentalo se i punti sono più distanti\n",
        "        max_nn=50     # Numero massimo di vicini per stimare la normale\n",
        "    ))\n",
        "\n",
        "    # Orienta le normali in modo consistente\n",
        "    pcd.orient_normals_consistent_tangent_plane(k=30)\n",
        "\n",
        "    # Ricostruzione con Ball Pivoting\n",
        "    mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_alpha_shape(\n",
        "        pcd, 0.15\n",
        "    )\n",
        "\n",
        "    # (Opzionale) Liscia la mesh per rimuovere artefatti\n",
        "    mesh = mesh.filter_smooth_laplacian(number_of_iterations=5)\n",
        "\n",
        "    # Salva e visualizza la mesh\n",
        "    o3d.io.write_triangle_mesh(\"alpha.obj\", mesh)"
      ],
      "metadata": {
        "id": "fBayJ77NDNZd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FUNZIONE"
      ],
      "metadata": {
        "id": "Ihm5O9Gho7bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize (vertex, targets , modelCat):\n",
        "\n",
        "    pointToMesh(vertex) #\n",
        "\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "    render_res = 224\n",
        "    n_iter = 1000\n",
        "    res = 224\n",
        "    obj_path = 'alpha.obj'\n",
        "    output_dir = './output/'\n",
        "    clip_model = 'ViT-L/14'\n",
        "\n",
        "    input_dim = 3\n",
        "    hidden_dim = 256\n",
        "    output_dim = 2\n",
        "\n",
        "    # Hyper-parameters\n",
        "    learning_rate = 0.0001\n",
        "    n_layers = 4 # depth 4\n",
        "    n_views = 3 # 4\n",
        "    n_augs = 3 # 4\n",
        "\n",
        "    Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "    render = Renderer(dim=(render_res, render_res))\n",
        "    mesh = Mesh(obj_path)\n",
        "    MeshNormalizer(mesh)()\n",
        "\n",
        "    # Initialize variables\n",
        "    background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "    log_dir = output_dir\n",
        "\n",
        "    # CLIP and augmentation transform\n",
        "    clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "    augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "    ])\n",
        "\n",
        "    # MLP Settings\n",
        "    mlp = NeuralHighlighter(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "    # list of possible colors\n",
        "    rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "    color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "    colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "    # --- Prompt ---\n",
        "    # encode prompt with CLIP\n",
        "    model = get_clip_model(clip_model)\n",
        "\n",
        "    known_object = modelCat\n",
        "\n",
        "    #prompt      = f\"A 3D render of a grey bowl with the part where a person can sit highlighted\"\n",
        "\n",
        "    # prompt = f\"A 3D render of a grey bowl being wrap-grasped, with the part of the bowl being wrap-grasped highlighted\"\n",
        "\n",
        "    prompt = f\"A 3D render of a grey bowl with the containing region highlighted\"\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prompt_token = clip.tokenize([prompt]).to(device)\n",
        "        encoded_text = model.encode_text(prompt_token)\n",
        "        encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "    vertices = copy.deepcopy(mesh.vertices)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in tqdm(range(n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        sampled_mesh = mesh\n",
        "        color_mesh(pred_class, sampled_mesh, colors)\n",
        "        rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                                show=False,\n",
        "                                                                center_azim=0,\n",
        "                                                                center_elev=0,\n",
        "                                                                std=1,\n",
        "                                                                return_views=True,\n",
        "                                                                lighting=True,\n",
        "                                                                background=background)\n",
        "\n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, model)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(log_dir, i, rendered_images)\n",
        "            with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "\n",
        "    # save results\n",
        "    save_final_results(log_dir, 'TTest2', mesh, mlp, vertices, colors, render, background)\n",
        "\n",
        "    return mlp\n"
      ],
      "metadata": {
        "id": "ezLSO9xoorXo"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "vt3qpl32MMrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, vertex, targets):\n",
        "    model.eval()\n",
        "\n",
        "    results = torch.zeros(\n",
        "        (1, 2048, 2))\n",
        "\n",
        "    vertex, targets = vertex.float().cuda(), targets.float().cuda()\n",
        "\n",
        "    afford_pred = torch.sigmoid(model(vertex))\n",
        "    afford_pred = afford_pred.permute(0, 2, 1).contiguous()\n",
        "\n",
        "    score = afford_pred.squeeze()\n",
        "    target_score = targets.squeeze()\n",
        "    results[0, :, :] = score\n",
        "    targets[0, :, :] = target_score\n",
        "\n",
        "    results = results.detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    IOU = np.zeros((targets.shape[0], targets.shape[2]))\n",
        "    targets = targets >= 0.5\n",
        "    targets = targets.astype(int)\n",
        "\n",
        "    IOU_thres = np.linspace(0, 1, 20)\n",
        "    for i in range(IOU.shape[0]):\n",
        "        t = targets[i, :, :]\n",
        "        p = results[i, :, :]\n",
        "        for j in range(t.shape[1]):\n",
        "            t_true = t[:, j]\n",
        "            p_score = p[:, j]\n",
        "            if np.sum(t_true) == 0:\n",
        "                IOU[i, j] = np.nan\n",
        "            else:\n",
        "                p_mask = (p_score > 0.5).astype(int)\n",
        "                temp_iou = []\n",
        "                for thre in IOU_thres:\n",
        "                    p_mask = (p_score >= thre).astype(int)\n",
        "                    intersect = np.sum(p_mask & t_true)\n",
        "                    union = np.sum(p_mask | t_true)\n",
        "                    temp_iou.append(1.*intersect/union)\n",
        "                temp_iou = np.array(temp_iou)\n",
        "                aiou = np.mean(temp_iou)\n",
        "                IOU[i, j] = aiou\n",
        "\n",
        "    IOU = np.nanmean(IOU, axis=0)\n",
        "\n",
        "    return np.mean(IOU)"
      ],
      "metadata": {
        "id": "gAEYmxxMEbsh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 2"
      ],
      "metadata": {
        "id": "XnGjH7ViMQbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_miou(predictions, ground_truths, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calcola il Mean Intersection Over Union (mIOU).\n",
        "\n",
        "    Args:\n",
        "        predictions (np.ndarray): Maschere predette, shape (B, N, C),\n",
        "                                   dove B è il batch, N i punti, C le affordance.\n",
        "        ground_truths (np.ndarray): Ground truth, shape (B, N, C).\n",
        "        threshold (float): Soglia per binarizzare le predizioni.\n",
        "\n",
        "    Returns:\n",
        "        float: mIOU medio su tutte le affordance e il batch.\n",
        "    \"\"\"\n",
        "    print(f\"Predictions shape: {predictions.shape}\")\n",
        "    print(f\"Ground truths shape: {ground_truths.shape}\")\n",
        "\n",
        "    predictions = (predictions >= threshold).astype(int)  # Binarizza le predizioni\n",
        "    ground_truths = (ground_truths >= threshold).astype(int)  # Binarizza ground truth\n",
        "\n",
        "    batch_size, num_points = ground_truths.shape\n",
        "    iou_per_class = np.zeros((batch_size, 1))\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        pred = predictions[b, :, 0]\n",
        "        gt = ground_truths[b, :]\n",
        "\n",
        "        # Calcola intersezione e unione\n",
        "        intersection = np.sum(pred * gt)\n",
        "        union = np.sum(pred + gt) - intersection\n",
        "\n",
        "        if union == 0:  # Evita divisione per zero\n",
        "            iou_per_class[b, 0] = np.nan  # Non valido se non ci sono punti\n",
        "        else:\n",
        "            iou_per_class[b, 0] = intersection / union\n",
        "\n",
        "\n",
        "    # Media su batch e classi\n",
        "    mean_iou = np.nanmean(iou_per_class)\n",
        "    return mean_iou"
      ],
      "metadata": {
        "id": "FouwK_oaHfrT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_instance = None\n",
        "targets = None\n",
        "vertex = None\n",
        "\n",
        "for batch_idx, batch in enumerate(loader[\"val_loader\"]):\n",
        "    #if batch_idx == 0:\n",
        "     #   continue\n",
        "    # Estraggo il primo elemento del batch\n",
        "    data, data1, targets, modelid, modelcat = batch   # con target = GT  ( area giusta da colorare )\n",
        "    vertex = data\n",
        "\n",
        "    model_instance = optimize(data, targets, modelcat)\n",
        "\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "1kxAaIDayBU3",
        "outputId": "a0da418b-1a24-44db-c2d8-13ced5fc5ef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/1000 [00:00<07:40,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 100 CLIP score: -0.7451171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 26/1000 [00:10<06:54,  2.35it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predizioni del modello\n",
        "vertex, targets = vertex.float().cuda(), targets.float().cuda()\n",
        "afford_pred = model_instance(vertex)\n",
        "afford_pred = torch.sigmoid(afford_pred).detach().cpu().numpy()  # Shape: [B, N, C]\n",
        "\n",
        "new_targets = targets[:, :, 1]\n",
        "\n",
        "#for value in new_targets[0]:\n",
        " #   print(value)\n",
        "\n",
        "# Ground truth\n",
        "ground_truth = new_targets.detach().cpu().numpy()  # Shape: [B, N, C]\n",
        "\n",
        "# Calcolo del mIOU\n",
        "miou = calculate_miou(afford_pred, ground_truth)\n",
        "print(f\"Mean IOU: {miou}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awajPzSxE18_",
        "outputId": "b3aa89d3-d965-4f4f-9710-997ee8cd3836",
        "collapsed": true
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions shape: (1, 2048, 2)\n",
            "Ground truths shape: (1, 2048)\n",
            "Mean IOU: 0.09765625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NrEYUN1No9fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Numero di oggetti nel validation loader\n",
        "num_val_objects = len(loader[\"val_loader\"].dataset)\n",
        "print(f\"Number of objects in val_loader: {num_val_objects}\")\n",
        "\n",
        "# Numero di oggetti nel test loader\n",
        "num_test_objects = len(loader[\"test_loader\"].dataset)\n",
        "print(f\"Number of objects in test_loader: {num_test_objects}\")\n"
      ],
      "metadata": {
        "id": "6WblDH0yFUJ8",
        "outputId": "f1eba8d4-58fa-48d8-fb79-a7bd97e2374e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of objects in val_loader: 5\n",
            "Number of objects in test_loader: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: domani:\n",
        "\n",
        "- OTTENERE UN MESH DI QUALITA' MAGGIORE  !!"
      ],
      "metadata": {
        "id": "ewkuWeluGnWL"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}